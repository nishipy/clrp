{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "light-weight-roberta-base-baseline-epoch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishipy/clrp/blob/main/light_weight_roberta_base_baseline_epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5tA-mkxXV5b"
      },
      "source": [
        "# Overview\n",
        "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
        "\n",
        "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish).\n",
        "\n",
        "In addition, we use the [stratified_kfold train dataset](https://www.kaggle.com/takeshikobayashi/commonlit-train-datasetfor) training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H_YG8xbXZGN"
      },
      "source": [
        "## Original notebook\n",
        "- Lightweight Roberta solution\n",
        "  - https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch\n",
        "- pretraied with MLM\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBSvZK-MXsRq"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtTPu3rhX22a"
      },
      "source": [
        "## Checking GPU status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4-zjkgTXyB7",
        "outputId": "351c337c-d561-45fb-9079-3846a8595e75"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul  4 07:32:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9niGsYeuYEgf"
      },
      "source": [
        "## Download dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JKMEPT1YBzO",
        "outputId": "76747b4e-79da-4ba7-d5f1-14e34262c790"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12zbNycNavJc"
      },
      "source": [
        "### kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMQCChEYIiJ"
      },
      "source": [
        "!mkdir -p /root/.kaggle/\n",
        "!cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LampuH0tawt4"
      },
      "source": [
        "### Competition dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pn015V0Yamu",
        "outputId": "61ea32b5-9330-444e-8abd-d7d6c238615b"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/\n",
        "!kaggle competitions download -c commonlitreadabilityprize -p ../input/commonlitreadabilityprize/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/train_stratiKfold.csv.zip ../input/commonlitreadabilityprize/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading sample_submission.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/108 [00:00<?, ?B/s]\n",
            "100% 108/108 [00:00<00:00, 182kB/s]\n",
            "Downloading test.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/6.79k [00:00<?, ?B/s]\n",
            "100% 6.79k/6.79k [00:00<00:00, 7.08MB/s]\n",
            "Downloading train.csv.zip to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 76.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SouT_7pYuvb",
        "outputId": "a5b537b0-24d3-4ed2-c802-8014e24179c3"
      },
      "source": [
        "!unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n",
        "!unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../input/commonlitreadabilityprize/train.csv.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/train.csv  \n",
            "Archive:  ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/train_stratiKfold.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjihJSBzZVt8",
        "outputId": "392f94cb-cc9a-4283-f53e-df085039bbbe"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission.csv  train.csv      train_stratiKfold.csv\n",
            "test.csv\t       train.csv.zip  train_stratiKfold.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujOl7iGa2-C"
      },
      "source": [
        "### Model pre-trained with MLM \n",
        "- Notebook\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "- Model data\n",
        "  - https://www.kaggle.com/maunish/clrp-roberta-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9EgsJFaa7UW",
        "outputId": "083ca1b9-1958-4e8f-c1df-dc8b389f600e"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/pretrained-model/\n",
        "!kaggle datasets download maunish/clrp-roberta-base -p ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clrp-roberta-base.zip to ../input/commonlitreadabilityprize/pretrained-model\n",
            "100% 3.00G/3.01G [00:24<00:00, 96.7MB/s]\n",
            "100% 3.01G/3.01G [00:24<00:00, 131MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KA2Tzd5bfnJ",
        "outputId": "73111b6b-2ccb-4626-c825-0ab37e790602"
      },
      "source": [
        "!unzip -o ../input/commonlitreadabilityprize/pretrained-model/clrp-roberta-base.zip -d ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../input/commonlitreadabilityprize/pretrained-model/clrp-roberta-base.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/merges.txt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/special_tokens_map.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/tokenizer_config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/vocab.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/text.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeCpZ6DEYH90"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOrw8yaddJV0",
        "outputId": "aea39f3e-6693-48dc-eecc-a6a9845dfaa1"
      },
      "source": [
        "!pip install transformers accelerate datasets"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.5MB/s \n",
            "\u001b[?25hCollecting accelerate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Collecting pyaml>=20.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers, pyaml, accelerate, xxhash, fsspec, datasets\n",
            "Successfully installed accelerate-0.3.0 datasets-1.8.0 fsspec-2021.6.1 huggingface-hub-0.0.12 pyaml-20.4.0 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYxy_hAXV5e"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6-8wniOeBzQ"
      },
      "source": [
        "# Set constant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQjr2-NPXV5f"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 6\n",
        "#NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 248\n",
        "#(eval_rmse, step_size)\n",
        "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "ROBERTA_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/\"\n",
        "TOKENIZER_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/\"\n",
        "#ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "#TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBgC9X-JOotk",
        "outputId": "7b72c848-a3f3-4b45-cd65-f91fa857ed28"
      },
      "source": [
        "EVAL_SCHEDULE[0][1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnzprhriHc7B"
      },
      "source": [
        "# Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNCjd5KEXV5f"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A95IJgC8WPLz"
      },
      "source": [
        "train_dfには、Stratified kfold済みのデータセットを利用する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-5y27aJXV5f"
      },
      "source": [
        "#Use stratified k-fold train dataset\n",
        "#train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
        "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train_stratiKfold.csv\")\n",
        "\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
        "submission_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR2QTnuIXV5g"
      },
      "source": [
        "#TokenizerはRoberta-baseと同じ\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "uq6O3LeXZ3Q9",
        "outputId": "91f77c20-dd92-4a3b-afeb-d9680e59bc8f"
      },
      "source": [
        "train_df[train_df['kfold']!=1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>kfold</th>\n",
              "      <th>bins</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>bf24448fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Anywhere there is a frontier, where there are ...</td>\n",
              "      <td>-1.866238</td>\n",
              "      <td>0.510911</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>7cad0f936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A great violinist, Ole Bull by name, visited t...</td>\n",
              "      <td>-0.578482</td>\n",
              "      <td>0.471768</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>91e87e7dc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hans stopped snoring and awoke at supper-time....</td>\n",
              "      <td>-0.186015</td>\n",
              "      <td>0.492731</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>20a9f9032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Government of the United States has viewed...</td>\n",
              "      <td>-1.391438</td>\n",
              "      <td>0.499195</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>daab29b47</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forty years ago women were given no representa...</td>\n",
              "      <td>-1.291128</td>\n",
              "      <td>0.531642</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>2827</td>\n",
              "      <td>d25b7c3aa</td>\n",
              "      <td>https://www.commonlit.org/texts/the-center-of-...</td>\n",
              "      <td>CC BY-NC-SA 2.0</td>\n",
              "      <td>The sun is a star, just like the other million...</td>\n",
              "      <td>-0.580631</td>\n",
              "      <td>0.457745</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2828</th>\n",
              "      <td>2829</td>\n",
              "      <td>3c1662f6d</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It was the northwest coast of Australia, the c...</td>\n",
              "      <td>-1.678689</td>\n",
              "      <td>0.493150</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2830</th>\n",
              "      <td>2831</td>\n",
              "      <td>64b635d77</td>\n",
              "      <td>https://www.africanstorybook.org/</td>\n",
              "      <td>CC BY 4.0</td>\n",
              "      <td>Once long ago, the birds had a meeting. They w...</td>\n",
              "      <td>0.639650</td>\n",
              "      <td>0.503652</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2831</th>\n",
              "      <td>2832</td>\n",
              "      <td>d6764322c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As an adult, I might learn new actions by taki...</td>\n",
              "      <td>1.024258</td>\n",
              "      <td>0.549119</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2832</th>\n",
              "      <td>2833</td>\n",
              "      <td>cf44e1a67</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Where do we feel empathy and how does it work?...</td>\n",
              "      <td>0.564179</td>\n",
              "      <td>0.486699</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2266 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0         id  ... kfold bins\n",
              "1              1  bf24448fb  ...     3    4\n",
              "2              2  7cad0f936  ...     2    6\n",
              "4              4  91e87e7dc  ...     2    7\n",
              "5              5  20a9f9032  ...     4    5\n",
              "6              6  daab29b47  ...     2    5\n",
              "...          ...        ...  ...   ...  ...\n",
              "2826        2827  d25b7c3aa  ...     2    6\n",
              "2828        2829  3c1662f6d  ...     2    4\n",
              "2830        2831  64b635d77  ...     2    9\n",
              "2831        2832  d6764322c  ...     3   10\n",
              "2832        2833  cf44e1a67  ...     2    9\n",
              "\n",
              "[2266 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5miW_QBacqW"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luovjtC0XV5g"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHVj3D05XV5g"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df        \n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "        #改行を消してみる。元のNotebookではここはコメントアウトされている\n",
        "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
        "        \n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
        "    \n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = 'max_length',            \n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask=True\n",
        "        )        \n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
        "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
        "        \n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)            \n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smttt02WXV5h"
      },
      "source": [
        "# Model\n",
        "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL6qw55sXV5h"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        #config.jsonに書いてある設定値を更新する\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})                       \n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
        "            \n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 512),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )        \n",
        "\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(768, 1)                        \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_output = self.roberta(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)        \n",
        "\n",
        "        # There are a total of 13 layers of hidden states.\n",
        "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
        "        # We take the hidden states from the last Roberta layer.\n",
        "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        # The number of cells is MAX_LEN.\n",
        "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
        "        # In order to condense hidden states of all cells to a context vector,\n",
        "        # we compute a weighted average of the hidden states of all cells.\n",
        "        # We compute the weight of each cell, using the attention neural network.\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "                \n",
        "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
        "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
        "        # Now we compute context_vector as the weighted average.\n",
        "        # context_vector.shape is BATCH_SIZE x 768\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
        "        \n",
        "        # Now we reduce the context vector to the prediction score.\n",
        "        return self.regressor(context_vector)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_BKdywXVDzk"
      },
      "source": [
        "## Define eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrZm0bLLXV5h"
      },
      "source": [
        "#MSEで評価\n",
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJsYZflrVBMr"
      },
      "source": [
        "## Define predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxqWSjMHXV5i"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdOQdEY7U-gl"
      },
      "source": [
        "### Define Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7fx532GXV5i"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    #EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "    #-> EVAL_SCHEDULE[0][1] = 16\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    #Epoch数だけ繰り返す\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            #https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning\n",
        "            #`optimizer.step()`の直後、`scheduler.step()`をすべてのバッチで呼び出して、学習率を更新します。\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            #eval_period(初期値は16）stepごとにRMSEを評価\n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                #EVAL_SCHEDULEに定義したrmseによって\n",
        "                #eval_periodを変更する\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                #ベストスコアを記録\n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "\n",
        "            #stepをインクリメント                                          \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1bmfWQiU4yT"
      },
      "source": [
        "## Create Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8nGcWc7XV5j"
      },
      "source": [
        "def create_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "    \n",
        "    roberta_parameters = named_parameters[:197]    \n",
        "    attention_parameters = named_parameters[199:203]\n",
        "    regressor_parameters = named_parameters[203:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "\n",
        "        lr = 2e-5\n",
        "\n",
        "        if layer_num >= 69:        \n",
        "            lr = 5e-5\n",
        "\n",
        "        if layer_num >= 133:\n",
        "            lr = 1e-4\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW8_ta8Shvbp"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ASAFbdqXV5k",
        "outputId": "3ac16480-a058-4294-e346-00be5c3b6fad"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "\n",
        "for fold in range(NUM_FOLDS): \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = f\"model_{fold + 1}.pth\"\n",
        "        \n",
        "    set_random_seed(SEED + fold)\n",
        "\n",
        "    #Stratified kfold train dataset用に修正\n",
        "    train_dataset = LitDataset(train_df[train_df['kfold']!=fold])    \n",
        "    val_dataset = LitDataset(train_df[train_df['kfold']==fold])    \n",
        "    \n",
        "    #https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "    \n",
        "    #random_seedは、Foldごとに変わる\n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)\n",
        "    #Schedulerには、get_cosine_schedule_with_warmupを使っている\n",
        "    #その他の選択肢: https://huggingface.co/transformers/main_classes/optimizer_schedules.html#schedules                        \n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=50)    \n",
        "    \n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
        "\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 7.0 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9574\n",
            "New best_val_rmse: 0.9574\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7136\n",
            "New best_val_rmse: 0.7136\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6355\n",
            "New best_val_rmse: 0.6355\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6183\n",
            "New best_val_rmse: 0.6183\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6522\n",
            "Still best_val_rmse: 0.6183 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.569\n",
            "New best_val_rmse: 0.569\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5937\n",
            "Still best_val_rmse: 0.569 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.4992\n",
            "New best_val_rmse: 0.4992\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 0 batch_num: 136 val_rmse: 0.4883\n",
            "New best_val_rmse: 0.4883\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 0 batch_num: 140 val_rmse: 0.5332\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 15 val_rmse: 0.5074\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 31 val_rmse: 0.5167\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 47 val_rmse: 0.5226\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 63 val_rmse: 0.4963\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 71 val_rmse: 0.5535\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 87 val_rmse: 0.6173\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 103 val_rmse: 0.5384\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 119 val_rmse: 0.523\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 135 val_rmse: 0.5348\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.5588\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.5178\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.4886\n",
            "Still best_val_rmse: 0.4883 (from epoch 0)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4858\n",
            "New best_val_rmse: 0.4858\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.5068\n",
            "Still best_val_rmse: 0.4858 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.49\n",
            "Still best_val_rmse: 0.4858 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4881\n",
            "Still best_val_rmse: 0.4858 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4924\n",
            "Still best_val_rmse: 0.4858 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4834\n",
            "New best_val_rmse: 0.4834\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4951\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.485\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4901\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4872\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4871\n",
            "Still best_val_rmse: 0.4834 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4799\n",
            "New best_val_rmse: 0.4799\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4832\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.494\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4884\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4864\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "4 steps took 1.73 seconds\n",
            "Epoch: 3 batch_num: 3 val_rmse: 0.4878\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 7 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4799 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 11 val_rmse: 0.4791\n",
            "New best_val_rmse: 0.4791\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 13 val_rmse: 0.4841\n",
            "Still best_val_rmse: 0.4791 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 17 val_rmse: 0.4779\n",
            "New best_val_rmse: 0.4779\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 19 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4779 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 21 val_rmse: 0.4777\n",
            "New best_val_rmse: 0.4777\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 23 val_rmse: 0.4777\n",
            "New best_val_rmse: 0.4777\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 3 batch_num: 25 val_rmse: 0.4806\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 29 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 31 val_rmse: 0.5026\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 47 val_rmse: 0.4871\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 51 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 3 batch_num: 55 val_rmse: 0.4872\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 59 val_rmse: 0.493\n",
            "Still best_val_rmse: 0.4777 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 67 val_rmse: 0.477\n",
            "New best_val_rmse: 0.477\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 3 batch_num: 69 val_rmse: 0.486\n",
            "Still best_val_rmse: 0.477 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 73 val_rmse: 0.4818\n",
            "Still best_val_rmse: 0.477 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 77 val_rmse: 0.4806\n",
            "Still best_val_rmse: 0.477 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 81 val_rmse: 0.4768\n",
            "New best_val_rmse: 0.4768\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 83 val_rmse: 0.473\n",
            "New best_val_rmse: 0.473\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 85 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 87 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 89 val_rmse: 0.4819\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 93 val_rmse: 0.4894\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 97 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 99 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 101 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 103 val_rmse: 0.4874\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 107 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 3 batch_num: 109 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 111 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 113 val_rmse: 0.4798\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 3 batch_num: 115 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 117 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 3 batch_num: 119 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 121 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 123 val_rmse: 0.4869\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 127 val_rmse: 0.4936\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 135 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 137 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.73 seconds\n",
            "Epoch: 4 batch_num: 0 val_rmse: 0.4839\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 4 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 6 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 8 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 10 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 12 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 14 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 16 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 18 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 20 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 22 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 24 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 4 batch_num: 26 val_rmse: 0.4744\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 28 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 30 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 32 val_rmse: 0.4845\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 36 val_rmse: 0.4908\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 4 batch_num: 44 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 4 batch_num: 46 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 48 val_rmse: 0.4816\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 52 val_rmse: 0.4823\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 56 val_rmse: 0.4785\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 58 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 60 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 4 batch_num: 62 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 64 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 66 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 4 batch_num: 68 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 70 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 72 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 74 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 76 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 78 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 80 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 82 val_rmse: 0.48\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 84 val_rmse: 0.4801\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 88 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 90 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 92 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 4 batch_num: 94 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 96 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 4 batch_num: 98 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 100 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 102 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 104 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 106 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 108 val_rmse: 0.4743\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 110 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 112 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 4 batch_num: 114 val_rmse: 0.4799\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 116 val_rmse: 0.4813\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 120 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 122 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 124 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 126 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 128 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 130 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 132 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 134 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 136 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 138 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 140 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.931 seconds\n",
            "Epoch: 5 batch_num: 1 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 3 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 5 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 7 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 5 batch_num: 9 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 11 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 13 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 15 val_rmse: 0.4743\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 17 val_rmse: 0.4745\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 19 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 21 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 23 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 25 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 27 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 29 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 31 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 33 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 35 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 37 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 39 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 41 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 43 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 45 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 5 batch_num: 47 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 49 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 5 batch_num: 51 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 53 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 55 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 57 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 59 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 61 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 63 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 65 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 67 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 69 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 71 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 73 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 75 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 77 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 79 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 81 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 83 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 85 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 87 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 89 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 91 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 93 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 95 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 97 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 99 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 101 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 5 batch_num: 103 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 5 batch_num: 105 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 107 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 109 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 111 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 113 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 115 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 117 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 119 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 5 batch_num: 121 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 5 batch_num: 123 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 125 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 127 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 129 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 131 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 133 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 135 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 137 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 139 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.473 (from epoch 3)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4729622619488559]\n",
            "Mean: 0.4729622619488559\n",
            "\n",
            "Fold 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.84 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.027\n",
            "New best_val_rmse: 1.027\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7094\n",
            "New best_val_rmse: 0.7094\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.8308\n",
            "Still best_val_rmse: 0.7094 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.696\n",
            "New best_val_rmse: 0.696\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6491\n",
            "New best_val_rmse: 0.6491\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5592\n",
            "New best_val_rmse: 0.5592\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5931\n",
            "Still best_val_rmse: 0.5592 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.53\n",
            "New best_val_rmse: 0.53\n",
            "\n",
            "16 steps took 6.5 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5266\n",
            "New best_val_rmse: 0.5266\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5692\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5285\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5269\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5378\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.6027\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.5683\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.6753\n",
            "Still best_val_rmse: 0.5266 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.5119\n",
            "New best_val_rmse: 0.5119\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.5437\n",
            "Still best_val_rmse: 0.5119 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.5092\n",
            "New best_val_rmse: 0.5092\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.5097\n",
            "Still best_val_rmse: 0.5092 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4894\n",
            "New best_val_rmse: 0.4894\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4855\n",
            "New best_val_rmse: 0.4855\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4881\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.5041\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.502\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4901\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4959\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.5046\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "16 steps took 6.32 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4904\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4901\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "8 steps took 3.3 seconds\n",
            "Epoch: 3 batch_num: 5 val_rmse: 0.5005\n",
            "Still best_val_rmse: 0.4855 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 21 val_rmse: 0.4852\n",
            "New best_val_rmse: 0.4852\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 3 batch_num: 25 val_rmse: 0.4911\n",
            "Still best_val_rmse: 0.4852 (from epoch 3)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 3 batch_num: 33 val_rmse: 0.484\n",
            "New best_val_rmse: 0.484\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 37 val_rmse: 0.4871\n",
            "Still best_val_rmse: 0.484 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 41 val_rmse: 0.4847\n",
            "Still best_val_rmse: 0.484 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 45 val_rmse: 0.4852\n",
            "Still best_val_rmse: 0.484 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 49 val_rmse: 0.4792\n",
            "New best_val_rmse: 0.4792\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 51 val_rmse: 0.4861\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 55 val_rmse: 0.4818\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 59 val_rmse: 0.4866\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 63 val_rmse: 0.4868\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 67 val_rmse: 0.4805\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 71 val_rmse: 0.4836\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 75 val_rmse: 0.4832\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 79 val_rmse: 0.4809\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 83 val_rmse: 0.4805\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 87 val_rmse: 0.4846\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 91 val_rmse: 0.4812\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 95 val_rmse: 0.4828\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 99 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 103 val_rmse: 0.4878\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 107 val_rmse: 0.4913\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 115 val_rmse: 0.4799\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 117 val_rmse: 0.4798\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 119 val_rmse: 0.4798\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 121 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4792 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 123 val_rmse: 0.4791\n",
            "New best_val_rmse: 0.4791\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 125 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4791 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 127 val_rmse: 0.4774\n",
            "New best_val_rmse: 0.4774\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 129 val_rmse: 0.4762\n",
            "New best_val_rmse: 0.4762\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 131 val_rmse: 0.4761\n",
            "New best_val_rmse: 0.4761\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 3 batch_num: 133 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.4761 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 135 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4761 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 139 val_rmse: 0.4803\n",
            "Still best_val_rmse: 0.4761 (from epoch 3)\n",
            "\n",
            "4 steps took 1.71 seconds\n",
            "Epoch: 4 batch_num: 2 val_rmse: 0.4836\n",
            "Still best_val_rmse: 0.4761 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 6 val_rmse: 0.4761\n",
            "New best_val_rmse: 0.4761\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 4 batch_num: 8 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 10 val_rmse: 0.4862\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 14 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 18 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 20 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 22 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 24 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 4 batch_num: 26 val_rmse: 0.4841\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 30 val_rmse: 0.4842\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 34 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 36 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 38 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 40 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 42 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 44 val_rmse: 0.4817\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 48 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 50 val_rmse: 0.4785\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 52 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 54 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 56 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 58 val_rmse: 0.4803\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 62 val_rmse: 0.4829\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 66 val_rmse: 0.4808\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 70 val_rmse: 0.4815\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 74 val_rmse: 0.4822\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 78 val_rmse: 0.4816\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 82 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 84 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 88 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 90 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 92 val_rmse: 0.4799\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 94 val_rmse: 0.4811\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 98 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 100 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 102 val_rmse: 0.4778\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 104 val_rmse: 0.4778\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 106 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 108 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 110 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 112 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 114 val_rmse: 0.4804\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 118 val_rmse: 0.4805\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 122 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 124 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 126 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 128 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 130 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 132 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 134 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 136 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 138 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 140 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.938 seconds\n",
            "Epoch: 5 batch_num: 1 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 3 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 5 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 7 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 9 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 11 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 13 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 15 val_rmse: 0.4789\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 17 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 19 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 21 val_rmse: 0.4789\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 23 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 25 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 27 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 29 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 31 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 33 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 5 batch_num: 35 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 37 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 5 batch_num: 39 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 41 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 43 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 45 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 47 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 49 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 51 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 53 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 55 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 57 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 59 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 61 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 63 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 65 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 67 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 69 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 71 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 73 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 75 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 77 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 79 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 81 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 83 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 85 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 87 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 89 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 91 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 93 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 95 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 97 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 99 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 101 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 103 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 105 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 107 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 109 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 111 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 113 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 115 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 117 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 119 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 121 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 123 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 125 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 127 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 129 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 131 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 133 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 135 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 137 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 5 batch_num: 139 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4761 (from epoch 4)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4729622619488559, 0.47605331248717914]\n",
            "Mean: 0.4745077872180175\n",
            "\n",
            "Fold 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.84 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9434\n",
            "New best_val_rmse: 0.9434\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8244\n",
            "New best_val_rmse: 0.8244\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6613\n",
            "New best_val_rmse: 0.6613\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.5819\n",
            "New best_val_rmse: 0.5819\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6071\n",
            "Still best_val_rmse: 0.5819 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5689\n",
            "New best_val_rmse: 0.5689\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5225\n",
            "New best_val_rmse: 0.5225\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.564\n",
            "Still best_val_rmse: 0.5225 (from epoch 0)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.4896\n",
            "New best_val_rmse: 0.4896\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 7 val_rmse: 0.5123\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 23 val_rmse: 0.495\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 31 val_rmse: 0.5122\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 47 val_rmse: 0.4885\n",
            "New best_val_rmse: 0.4885\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.4973\n",
            "Still best_val_rmse: 0.4885 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 59 val_rmse: 0.5424\n",
            "Still best_val_rmse: 0.4885 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 75 val_rmse: 0.4653\n",
            "New best_val_rmse: 0.4653\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 1 batch_num: 76 val_rmse: 0.4722\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 1 batch_num: 78 val_rmse: 0.4979\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 86 val_rmse: 0.5088\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 102 val_rmse: 0.5087\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 118 val_rmse: 0.4906\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 126 val_rmse: 0.5605\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 2 batch_num: 1 val_rmse: 0.5656\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 17 val_rmse: 0.5058\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 33 val_rmse: 0.5011\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 49 val_rmse: 0.4706\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 51 val_rmse: 0.4708\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 53 val_rmse: 0.4689\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 56 val_rmse: 0.4883\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4832\n",
            "Still best_val_rmse: 0.4653 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.4611\n",
            "New best_val_rmse: 0.4611\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 65 val_rmse: 0.4604\n",
            "New best_val_rmse: 0.4604\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 2 batch_num: 67 val_rmse: 0.4666\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 69 val_rmse: 0.4672\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4681\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 71 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 73 val_rmse: 0.4975\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 81 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 85 val_rmse: 0.5054\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 2 batch_num: 101 val_rmse: 0.4672\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 103 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 104 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4961\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 115 val_rmse: 0.4645\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4652\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 117 val_rmse: 0.4633\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4646\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 119 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4969\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4712\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4851\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4683\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 137 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4633\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 139 val_rmse: 0.4626\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.469\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.576 seconds\n",
            "Epoch: 3 batch_num: 0 val_rmse: 0.4785\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 3 batch_num: 2 val_rmse: 0.4905\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 10 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 12 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 13 val_rmse: 0.4629\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 14 val_rmse: 0.4611\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 3 batch_num: 15 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 16 val_rmse: 0.4621\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 17 val_rmse: 0.4642\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 18 val_rmse: 0.4645\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 3 batch_num: 19 val_rmse: 0.467\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 3 batch_num: 20 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 22 val_rmse: 0.4817\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 26 val_rmse: 0.4708\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 3 batch_num: 28 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 29 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 30 val_rmse: 0.4689\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 31 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 3 batch_num: 33 val_rmse: 0.4744\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 35 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 36 val_rmse: 0.4613\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 3 batch_num: 37 val_rmse: 0.4599\n",
            "New best_val_rmse: 0.4599\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 3 batch_num: 38 val_rmse: 0.4587\n",
            "New best_val_rmse: 0.4587\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 39 val_rmse: 0.4589\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 40 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 41 val_rmse: 0.467\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 42 val_rmse: 0.4652\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 43 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 44 val_rmse: 0.4684\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 45 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 47 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 3 batch_num: 49 val_rmse: 0.4651\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 50 val_rmse: 0.4618\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 51 val_rmse: 0.46\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 52 val_rmse: 0.4607\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 53 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 54 val_rmse: 0.4631\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 55 val_rmse: 0.4698\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 3 batch_num: 56 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 58 val_rmse: 0.4921\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 66 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 67 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 69 val_rmse: 0.4913\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 77 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 78 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 79 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 81 val_rmse: 0.489\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 85 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 87 val_rmse: 0.4645\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 88 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 89 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 90 val_rmse: 0.4607\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 91 val_rmse: 0.461\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 3 batch_num: 92 val_rmse: 0.4625\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 93 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 94 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 95 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 96 val_rmse: 0.4601\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 97 val_rmse: 0.4606\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 98 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 99 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 100 val_rmse: 0.4624\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 3 batch_num: 101 val_rmse: 0.4677\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 102 val_rmse: 0.4826\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 106 val_rmse: 0.4889\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 110 val_rmse: 0.4667\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 111 val_rmse: 0.4694\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 112 val_rmse: 0.4625\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 113 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 114 val_rmse: 0.4711\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 116 val_rmse: 0.4951\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 124 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 125 val_rmse: 0.465\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 126 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 3 batch_num: 127 val_rmse: 0.4624\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 128 val_rmse: 0.4733\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 130 val_rmse: 0.4937\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 3 batch_num: 138 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 3 batch_num: 139 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.39 seconds\n",
            "Epoch: 3 batch_num: 140 val_rmse: 0.4635\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.562 seconds\n",
            "Epoch: 4 batch_num: 0 val_rmse: 0.4674\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 4 batch_num: 1 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 4 batch_num: 3 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 7 val_rmse: 0.4688\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 8 val_rmse: 0.4651\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 9 val_rmse: 0.4629\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 10 val_rmse: 0.4618\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 11 val_rmse: 0.461\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 12 val_rmse: 0.4607\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 13 val_rmse: 0.4619\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 14 val_rmse: 0.4648\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 15 val_rmse: 0.47\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 17 val_rmse: 0.4806\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 21 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 23 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 24 val_rmse: 0.4591\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 25 val_rmse: 0.4597\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 4 batch_num: 26 val_rmse: 0.4604\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 4 batch_num: 27 val_rmse: 0.4606\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 4 batch_num: 28 val_rmse: 0.46\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 29 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 30 val_rmse: 0.4689\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 31 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 33 val_rmse: 0.4897\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 37 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 39 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 40 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 41 val_rmse: 0.4605\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 42 val_rmse: 0.46\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 43 val_rmse: 0.4604\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 44 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 45 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 46 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 47 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 48 val_rmse: 0.4671\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 49 val_rmse: 0.4677\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 50 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 51 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 52 val_rmse: 0.4665\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 53 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 54 val_rmse: 0.4665\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 55 val_rmse: 0.4683\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 56 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 58 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 60 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 62 val_rmse: 0.4666\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 63 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 64 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 65 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 66 val_rmse: 0.4671\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 67 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 68 val_rmse: 0.467\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 69 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 4 batch_num: 70 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 71 val_rmse: 0.4639\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 72 val_rmse: 0.4631\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 73 val_rmse: 0.4626\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 74 val_rmse: 0.4624\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 75 val_rmse: 0.4633\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 76 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 77 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 78 val_rmse: 0.4651\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 79 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 80 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 81 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 82 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 83 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 84 val_rmse: 0.4643\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 85 val_rmse: 0.4629\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 86 val_rmse: 0.4618\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 87 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 88 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 89 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 90 val_rmse: 0.4616\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 91 val_rmse: 0.4626\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 92 val_rmse: 0.464\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 93 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 94 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 4 batch_num: 95 val_rmse: 0.4715\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 97 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 99 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 101 val_rmse: 0.4724\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 103 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 4 batch_num: 104 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 105 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 4 batch_num: 106 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 107 val_rmse: 0.4613\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 4 batch_num: 108 val_rmse: 0.4613\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 109 val_rmse: 0.4611\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 4 batch_num: 110 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 111 val_rmse: 0.4619\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 112 val_rmse: 0.4629\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 113 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 114 val_rmse: 0.465\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 115 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 116 val_rmse: 0.4667\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 117 val_rmse: 0.4678\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 118 val_rmse: 0.469\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 119 val_rmse: 0.4703\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 121 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 123 val_rmse: 0.4743\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 125 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 127 val_rmse: 0.4692\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 128 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 129 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 130 val_rmse: 0.465\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 131 val_rmse: 0.4648\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 132 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 133 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 4 batch_num: 134 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 135 val_rmse: 0.464\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 136 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 4 batch_num: 137 val_rmse: 0.4642\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 4 batch_num: 138 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 139 val_rmse: 0.4649\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 4 batch_num: 140 val_rmse: 0.4649\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.555 seconds\n",
            "Epoch: 5 batch_num: 0 val_rmse: 0.4646\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 1 val_rmse: 0.4643\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 2 val_rmse: 0.464\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 3 val_rmse: 0.4633\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 4 val_rmse: 0.4627\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 5 val_rmse: 0.4623\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 6 val_rmse: 0.4619\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 7 val_rmse: 0.4621\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 8 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 9 val_rmse: 0.4621\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 10 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 11 val_rmse: 0.4623\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 12 val_rmse: 0.4627\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 13 val_rmse: 0.4627\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 14 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 15 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 5 batch_num: 16 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 17 val_rmse: 0.4639\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 18 val_rmse: 0.4642\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 19 val_rmse: 0.4646\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 20 val_rmse: 0.4651\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 21 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 22 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 23 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 24 val_rmse: 0.4675\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 25 val_rmse: 0.4679\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 26 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 27 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 28 val_rmse: 0.4683\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 29 val_rmse: 0.4683\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 30 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 5 batch_num: 31 val_rmse: 0.4677\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 5 batch_num: 32 val_rmse: 0.4672\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 33 val_rmse: 0.4667\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 34 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 35 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 36 val_rmse: 0.4652\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 37 val_rmse: 0.4646\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 38 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 39 val_rmse: 0.4636\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 40 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 41 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 42 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 43 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 44 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 45 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 46 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 47 val_rmse: 0.4636\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 48 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 49 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 50 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 51 val_rmse: 0.4648\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 52 val_rmse: 0.465\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 53 val_rmse: 0.4654\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 54 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 55 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 56 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 57 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 58 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 59 val_rmse: 0.4665\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 60 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 61 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 62 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 63 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 64 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 65 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 66 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 67 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 68 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 69 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 70 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 71 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 72 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 73 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 74 val_rmse: 0.4656\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 75 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 76 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 77 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 78 val_rmse: 0.4657\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 79 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 80 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 81 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 82 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 83 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 84 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 85 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 86 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 87 val_rmse: 0.4664\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 88 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 89 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 90 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 91 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 92 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 93 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 94 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 95 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 96 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 97 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 98 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 99 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 100 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 101 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 102 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 103 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 104 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 105 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.396 seconds\n",
            "Epoch: 5 batch_num: 106 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 107 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 108 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 109 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 110 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 111 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 112 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 113 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 114 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 115 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 116 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 117 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 118 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 119 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 120 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 121 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 122 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 5 batch_num: 123 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 124 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 125 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 126 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 127 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 128 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 129 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 130 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 5 batch_num: 131 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 132 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 133 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 134 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 5 batch_num: 135 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 136 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 5 batch_num: 137 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 5 batch_num: 138 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 5 batch_num: 139 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 5 batch_num: 140 val_rmse: 0.4661\n",
            "Still best_val_rmse: 0.4587 (from epoch 3)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4729622619488559, 0.47605331248717914, 0.4587270785429549]\n",
            "Mean: 0.4692475509929966\n",
            "\n",
            "Fold 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.84 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.059\n",
            "New best_val_rmse: 1.059\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.9468\n",
            "New best_val_rmse: 0.9468\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6678\n",
            "New best_val_rmse: 0.6678\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.7192\n",
            "Still best_val_rmse: 0.6678 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.5945\n",
            "New best_val_rmse: 0.5945\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5726\n",
            "New best_val_rmse: 0.5726\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6097\n",
            "Still best_val_rmse: 0.5726 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5149\n",
            "New best_val_rmse: 0.5149\n",
            "\n",
            "16 steps took 6.5 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5729\n",
            "Still best_val_rmse: 0.5149 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.4916\n",
            "New best_val_rmse: 0.4916\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 27 val_rmse: 0.502\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 43 val_rmse: 0.5764\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 59 val_rmse: 0.5267\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 75 val_rmse: 0.5012\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 91 val_rmse: 0.5035\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 107 val_rmse: 0.5032\n",
            "Still best_val_rmse: 0.4916 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 123 val_rmse: 0.4808\n",
            "New best_val_rmse: 0.4808\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 127 val_rmse: 0.4778\n",
            "New best_val_rmse: 0.4778\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 1 batch_num: 129 val_rmse: 0.524\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 2 batch_num: 4 val_rmse: 0.5557\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 20 val_rmse: 0.4916\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 28 val_rmse: 0.4939\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 36 val_rmse: 0.4867\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 40 val_rmse: 0.4843\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 44 val_rmse: 0.4907\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 52 val_rmse: 0.4956\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4993\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4887\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4877\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.5274\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 92 val_rmse: 0.4893\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 96 val_rmse: 0.4849\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.4975\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 108 val_rmse: 0.539\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.505\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.5065\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.5 seconds\n",
            "Epoch: 3 batch_num: 15 val_rmse: 0.513\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 31 val_rmse: 0.5027\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 47 val_rmse: 0.4808\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 51 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 55 val_rmse: 0.486\n",
            "Still best_val_rmse: 0.4778 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 59 val_rmse: 0.4761\n",
            "New best_val_rmse: 0.4761\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 61 val_rmse: 0.4737\n",
            "New best_val_rmse: 0.4737\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 63 val_rmse: 0.4824\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 67 val_rmse: 0.5163\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 83 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 3 batch_num: 85 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 3 batch_num: 87 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 91 val_rmse: 0.4817\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 95 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 3 batch_num: 97 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 99 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 101 val_rmse: 0.4844\n",
            "Still best_val_rmse: 0.4737 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 105 val_rmse: 0.4735\n",
            "New best_val_rmse: 0.4735\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 3 batch_num: 107 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 109 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 111 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 115 val_rmse: 0.4853\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 119 val_rmse: 0.481\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 123 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 125 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 127 val_rmse: 0.4818\n",
            "Still best_val_rmse: 0.4735 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 3 batch_num: 131 val_rmse: 0.4733\n",
            "New best_val_rmse: 0.4733\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 133 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.4733 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 135 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4733 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 3 batch_num: 137 val_rmse: 0.4719\n",
            "New best_val_rmse: 0.4719\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 3 batch_num: 139 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.94 seconds\n",
            "Epoch: 4 batch_num: 0 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 4 batch_num: 2 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 4 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 6 val_rmse: 0.4846\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 10 val_rmse: 0.4822\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 14 val_rmse: 0.4828\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 18 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 20 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 22 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 24 val_rmse: 0.4722\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 26 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 28 val_rmse: 0.4822\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 32 val_rmse: 0.4863\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 36 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 38 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 40 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 42 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 4 batch_num: 44 val_rmse: 0.4805\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 48 val_rmse: 0.4817\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 52 val_rmse: 0.4823\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 56 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.4719 (from epoch 3)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 58 val_rmse: 0.471\n",
            "New best_val_rmse: 0.471\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 60 val_rmse: 0.4712\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 62 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 64 val_rmse: 0.4823\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 68 val_rmse: 0.4897\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 72 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 74 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 76 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 78 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 80 val_rmse: 0.4839\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 84 val_rmse: 0.4879\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 88 val_rmse: 0.4815\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 92 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 94 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 96 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 98 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 100 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 102 val_rmse: 0.4803\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 4 batch_num: 106 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 108 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 110 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 112 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 4 batch_num: 114 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 116 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 118 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 120 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 122 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 124 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 126 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 4 batch_num: 128 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 4 batch_num: 130 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 132 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 4 batch_num: 134 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 4 batch_num: 136 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 138 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 4 batch_num: 140 val_rmse: 0.4745\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.949 seconds\n",
            "Epoch: 5 batch_num: 1 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 3 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 5 batch_num: 5 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 7 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 9 val_rmse: 0.4803\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 5 batch_num: 13 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 15 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 5 batch_num: 17 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 19 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 21 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 5 batch_num: 23 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 25 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 27 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 29 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 31 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 33 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 35 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 37 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 39 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 41 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 43 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 45 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 47 val_rmse: 0.4801\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 5 batch_num: 51 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 5 batch_num: 55 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 57 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 59 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 61 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 63 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 65 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 67 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 69 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 71 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 73 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 75 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 77 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 79 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 81 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 83 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 85 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 87 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 89 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 91 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 5 batch_num: 93 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 95 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 97 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 99 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 101 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 103 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 105 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 107 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 5 batch_num: 109 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 111 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 5 batch_num: 113 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 115 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 5 batch_num: 117 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 119 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 121 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 5 batch_num: 123 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 5 batch_num: 125 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 127 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 129 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 131 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 133 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 135 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 5 batch_num: 137 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 5 batch_num: 139 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.471 (from epoch 4)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4729622619488559, 0.47605331248717914, 0.4587270785429549, 0.47097710716964525]\n",
            "Mean: 0.46967994003715874\n",
            "\n",
            "Fold 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.85 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.8846\n",
            "New best_val_rmse: 0.8846\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7855\n",
            "New best_val_rmse: 0.7855\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6569\n",
            "New best_val_rmse: 0.6569\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6467\n",
            "New best_val_rmse: 0.6467\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6277\n",
            "New best_val_rmse: 0.6277\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5707\n",
            "New best_val_rmse: 0.5707\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5704\n",
            "New best_val_rmse: 0.5704\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5402\n",
            "New best_val_rmse: 0.5402\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5268\n",
            "New best_val_rmse: 0.5268\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.6124\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5579\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5387\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5701\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.5333\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.557\n",
            "Still best_val_rmse: 0.5268 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.5222\n",
            "New best_val_rmse: 0.5222\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.6762\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.545\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.5493\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.5308\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.5435\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.5342\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.5261\n",
            "Still best_val_rmse: 0.5222 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.5136\n",
            "New best_val_rmse: 0.5136\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.518\n",
            "Still best_val_rmse: 0.5136 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.5194\n",
            "Still best_val_rmse: 0.5136 (from epoch 2)\n",
            "\n",
            "16 steps took 6.5 seconds\n",
            "Epoch: 3 batch_num: 9 val_rmse: 0.5515\n",
            "Still best_val_rmse: 0.5136 (from epoch 2)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 25 val_rmse: 0.5109\n",
            "New best_val_rmse: 0.5109\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 41 val_rmse: 0.5199\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 57 val_rmse: 0.526\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 73 val_rmse: 0.5194\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 89 val_rmse: 0.5215\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 105 val_rmse: 0.5219\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 121 val_rmse: 0.5114\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 3 batch_num: 137 val_rmse: 0.5251\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 4 batch_num: 12 val_rmse: 0.5133\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 28 val_rmse: 0.5232\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 44 val_rmse: 0.5154\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 60 val_rmse: 0.5179\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 76 val_rmse: 0.5194\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 92 val_rmse: 0.5187\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 108 val_rmse: 0.5157\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 124 val_rmse: 0.5176\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 4 batch_num: 140 val_rmse: 0.5218\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 5 batch_num: 15 val_rmse: 0.5133\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 31 val_rmse: 0.5192\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 47 val_rmse: 0.5189\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 63 val_rmse: 0.5184\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 79 val_rmse: 0.5172\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 95 val_rmse: 0.5173\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 111 val_rmse: 0.5172\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 5 batch_num: 127 val_rmse: 0.5172\n",
            "Still best_val_rmse: 0.5109 (from epoch 3)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4729622619488559, 0.47605331248717914, 0.4587270785429549, 0.47097710716964525, 0.5108580734790957]\n",
            "Mean: 0.4779155667255461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9PqamPNXV5k"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bAMT9-NXV5k"
      },
      "source": [
        "test_dataset = LitDataset(test_df, inference_only=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5Y_L0yfXV5k",
        "outputId": "a4d8f103-3137-4fb0-9bb8-c27eeb0037bb"
      },
      "source": [
        "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
        "\n",
        "test_dataset = LitDataset(test_df, inference_only=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                         drop_last=False, shuffle=False, num_workers=2)\n",
        "\n",
        "for index in range(len(list_val_rmse)):            \n",
        "    model_path = f\"model_{index + 1}.pth\"\n",
        "    print(f\"\\nUsing {model_path}\")\n",
        "                        \n",
        "    model = LitModel()\n",
        "    model.load_state_dict(torch.load(model_path))    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    all_predictions[index] = predict(model, test_loader)\n",
        "    \n",
        "    del model\n",
        "    gc.collect()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_1.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_2.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_3.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_4.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUDbpVq8XV5l"
      },
      "source": [
        "predictions = all_predictions.mean(axis=0)\n",
        "submission_df.target = predictions\n",
        "print(submission_df)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXCmIyftjlEY"
      },
      "source": [
        "# Upload data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3scDIDWzJu_X"
      },
      "source": [
        "!date +\"%Y%m%d%I%M%S\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8IUhtyYjmso",
        "outputId": "63d36e36-26ce-4456-b34f-d2903a07edc4"
      },
      "source": [
        "!mkdir -p ./output/\n",
        "!cp -f ./model* ./output/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/Lightweight-Roberta-base/dataset-metadata-epoch.json ./output/\n",
        "!sed -i -e \"s/lightweight-roberta-base/lightweight-roberta-base-`date +\"%Y%m%d%I%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p ./output/"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset slug must be between 6 and 50 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWMDce0OKDuQ"
      },
      "source": [
        "!cat ./output/dataset-metadata.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoMWryqK81cN",
        "outputId": "f938d7b2-2566-4209-b98a-6f112b4627a3"
      },
      "source": [
        "!mkdir -p ./output/\n",
        "!cp -f ./model* ./output/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/Lightweight-Roberta-base/dataset-metadata-epoch.json ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/lightweight-roberta-base/lightweight-roberta-base-`date +\"%Y%m%d%I%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p ./output/"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting upload for file model_3.pth\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 477M/477M [00:10<00:00, 47.8MB/s]\n",
            "Upload successful: model_3.pth (477MB)\n",
            "Starting upload for file model_5.pth\n",
            "100% 477M/477M [00:11<00:00, 44.0MB/s]\n",
            "Upload successful: model_5.pth (477MB)\n",
            "Starting upload for file model_4.pth\n",
            "100% 477M/477M [00:10<00:00, 49.5MB/s]\n",
            "Upload successful: model_4.pth (477MB)\n",
            "Starting upload for file model_2.pth\n",
            "100% 477M/477M [00:12<00:00, 40.6MB/s]\n",
            "Upload successful: model_2.pth (477MB)\n",
            "Starting upload for file dataset-metadata-epoch.json\n",
            "100% 174/174 [00:01<00:00, 132B/s]\n",
            "Upload successful: dataset-metadata-epoch.json (174B)\n",
            "Starting upload for file model_1.pth\n",
            "100% 477M/477M [00:11<00:00, 45.4MB/s]\n",
            "Upload successful: model_1.pth (477MB)\n",
            "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//iamnishipy/lightweight-roberta-base-20210704095744-epoch\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
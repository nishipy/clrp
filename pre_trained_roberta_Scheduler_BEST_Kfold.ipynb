{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2702.979398,
      "end_time": "2021-07-10T17:19:56.452391",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-10T16:34:53.472993",
      "version": "2.3.3"
    },
    "colab": {
      "name": "pre-trained-roberta-Scheduler-BEST-Kfold.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "03O1cJpp9HAO"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbX1a6A8Doq"
      },
      "source": [
        "# Prerequisite"
      ],
      "id": "UrbX1a6A8Doq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDxPgc_8SBn",
        "outputId": "ce46da1d-81cc-4cff-b481-e5f94d8a2fb3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "YdDxPgc_8SBn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-21rfg88O1A",
        "outputId": "890fefc8-8d0b-4332-f49c-ae82324549fd"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "Q-21rfg88O1A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 29 16:12:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlrHGNqyOVEG",
        "outputId": "2a23875b-97e8-4aff-d30c-c87f0a560542"
      },
      "source": [
        "!python -V"
      ],
      "id": "jlrHGNqyOVEG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnXDSalTBOqs"
      },
      "source": [
        ""
      ],
      "id": "JnXDSalTBOqs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEBNoyjUN62w",
        "outputId": "96522ef3-b9f5-4fda-d8c4-da52d11e03a1"
      },
      "source": [
        "# !pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"
      ],
      "id": "OEBNoyjUN62w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mpmath==1.2.1\n",
            "numpy==1.19.5\n",
            "pandas==1.1.5\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "tensorflow-gcs-config==2.5.0\n",
            "torch @ https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp37-cp37m-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.10.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu102/torchvision-0.10.0%2Bcu102-cp37-cp37m-linux_x86_64.whl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoRvT25dQZfN"
      },
      "source": [
        "(On kaggle notebook)\n",
        "```\n",
        "gcsfs @ file:///home/conda/feedstock_root/build_artifacts/gcsfs_1613013312379/work\n",
        "geopandas @ file:///home/conda/feedstock_root/build_artifacts/geopandas_1614589434390/work\n",
        "grpcio-gcp @ file:///home/conda/feedstock_root/build_artifacts/grpcio-gcp_1604414757985/work\n",
        "hpsklearn==0.1.0\n",
        "mpmath==1.2.1\n",
        "msgpack-numpy==0.4.7.1\n",
        "numpy==1.19.5\n",
        "pandas==1.2.3\n",
        "pandas-datareader==0.9.0\n",
        "pandas-profiling @ file:///home/conda/feedstock_root/build_artifacts/pandas-profiling_1613839428900/work\n",
        "pandas-summary==0.0.7\n",
        "pandasql==0.7.3\n",
        "pytorch-ignite==0.4.4\n",
        "pytorch-lightning==1.2.8\n",
        "randomgen==1.16.6\n",
        "sklearn==0.0\n",
        "sklearn-contrib-py-earth @ git+git://github.com/scikit-learn-contrib/py-earth.git@dde5f899255411a7b9cbbabf93a817eff4b02e5e\n",
        "sklearn-pandas==2.1.0\n",
        "tensorflow-gcs-config @ file:///tmp/tensorflow_gcs_config/tensorflow_gcs_config-2.1.7-py3-none-any.whl\n",
        "torch==1.7.0\n",
        "torchaudio==0.7.0a0+ac17b64\n",
        "torchmetrics==0.2.0\n",
        "torchtext==0.8.0a0+cd6902d\n",
        "torchvision==0.8.1\n",
        "transformers==4.5.1\n",
        "```"
      ],
      "id": "BoRvT25dQZfN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBS2Zv7DR2tk",
        "outputId": "308a56c6-9e44-4a0b-86da-7d67b454468c"
      },
      "source": [
        "!cp -f /content/drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/requirements.txt ./\n",
        "!cat ./requirements.txt"
      ],
      "id": "LBS2Zv7DR2tk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pandas==1.2.3\n",
            "sklearn==0.0\n",
            "sklearn-pandas==2.1.0\n",
            "torch==1.7.0\n",
            "torchmetrics==0.2.0\n",
            "#torchtext==0.8.0a0+cd6902d\n",
            "torchvision==0.8.1\n",
            "transformers==4.5.1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7y1hmfoS-Q1",
        "outputId": "f5a04219-5368-453d-bfe0-f4b23b34c4ff"
      },
      "source": [
        "!pip uninstall -r ./requirements.txt -y"
      ],
      "id": "k7y1hmfoS-Q1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: pandas 1.1.5\n",
            "Uninstalling pandas-1.1.5:\n",
            "  Successfully uninstalled pandas-1.1.5\n",
            "Found existing installation: sklearn 0.0\n",
            "Uninstalling sklearn-0.0:\n",
            "  Successfully uninstalled sklearn-0.0\n",
            "Found existing installation: sklearn-pandas 1.8.0\n",
            "Uninstalling sklearn-pandas-1.8.0:\n",
            "  Successfully uninstalled sklearn-pandas-1.8.0\n",
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[33mWARNING: Skipping torchmetrics as it is not installed.\u001b[0m\n",
            "Found existing installation: torchvision 0.10.0+cu102\n",
            "Uninstalling torchvision-0.10.0+cu102:\n",
            "  Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DhM67RrVQYws",
        "outputId": "fd8dba73-933b-463d-9c5c-d2040fefcc9b"
      },
      "source": [
        "# !pip install -r ./requirements.txt "
      ],
      "id": "DhM67RrVQYws",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.2.3\n",
            "  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting sklearn==0.0\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting sklearn-pandas==2.1.0\n",
            "  Downloading sklearn_pandas-2.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.0 kB/s \n",
            "\u001b[?25hCollecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.1\n",
            "  Downloading torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 21.3 MB/s \n",
            "\u001b[?25hCollecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 91.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r ./requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r ./requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r ./requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r ./requirements.txt (line 2)) (0.22.2.post1)\n",
            "Collecting scipy>=1.5.1\n",
            "  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r ./requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r ./requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1->-r ./requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r ./requirements.txt (line 8)) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3->-r ./requirements.txt (line 1)) (1.15.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r ./requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1->-r ./requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1->-r ./requirements.txt (line 8)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r ./requirements.txt (line 8)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r ./requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r ./requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r ./requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r ./requirements.txt (line 8)) (7.1.2)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=985d51a88c27523058c9d19ed79c741be2e16cbbdfc230de1f4da4b647e2cacc\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: threadpoolctl, scipy, dataclasses, torch, tokenizers, scikit-learn, sacremoses, pandas, transformers, torchvision, torchmetrics, sklearn-pandas, sklearn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 pandas-1.2.3 sacremoses-0.0.45 scikit-learn-0.24.2 scipy-1.7.0 sklearn-0.0 sklearn-pandas-2.1.0 threadpoolctl-2.2.0 tokenizers-0.10.3 torch-1.7.0 torchmetrics-0.2.0 torchvision-0.8.1 transformers-4.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBMkIBcxUIb2",
        "outputId": "30731a5e-c1c5-492b-f28f-94bc23fa7907"
      },
      "source": [
        "# !pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"
      ],
      "id": "HBMkIBcxUIb2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mpmath==1.2.1\n",
            "numpy==1.19.5\n",
            "pandas==1.2.3\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "sklearn==0.0\n",
            "sklearn-pandas==2.1.0\n",
            "tensorflow-gcs-config==2.5.0\n",
            "torch==1.7.0\n",
            "torchmetrics==0.2.0\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.10.0\n",
            "torchvision==0.8.1\n",
            "transformers==4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgWJyvxHUAMD"
      },
      "source": [
        ""
      ],
      "id": "QgWJyvxHUAMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2yfzg48VPx"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "De2yfzg48VPx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZk0yJH8mWO"
      },
      "source": [
        "### kaggle.json"
      ],
      "id": "_YZk0yJH8mWO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEuB9fOD8j8l"
      },
      "source": [
        "!mkdir -p /root/.kaggle/\n",
        "!cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "DEuB9fOD8j8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKBPd9H8q0L"
      },
      "source": [
        "### Competition dataset"
      ],
      "id": "4oKBPd9H8q0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv4fLHsg8vsO",
        "outputId": "714f65c4-36b5-4609-8889-86e7c77c92fd"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/\n",
        "!kaggle competitions download -c commonlitreadabilityprize -p ../input/commonlitreadabilityprize/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/cpp_stratiKfold.csv ../input/commonlitreadabilityprize/"
      ],
      "id": "lv4fLHsg8vsO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/6.79k [00:00<?, ?B/s]\n",
            "100% 6.79k/6.79k [00:00<00:00, 11.0MB/s]\n",
            "Downloading sample_submission.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/108 [00:00<?, ?B/s]\n",
            "100% 108/108 [00:00<00:00, 110kB/s]\n",
            "Downloading train.csv.zip to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 76.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1QcZXR79EO2",
        "outputId": "df64aae9-a15b-429e-936c-d07030ad1807"
      },
      "source": [
        "!unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n",
        "#!unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"
      ],
      "id": "R1QcZXR79EO2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../input/commonlitreadabilityprize/train.csv.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YesBgkTbV23Z"
      },
      "source": [
        "!cp ./drive/MyDrive/kaggle/commonlit/train_folds_thakur.csv ../input/commonlitreadabilityprize/"
      ],
      "id": "YesBgkTbV23Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJaSyc89GiB",
        "outputId": "63bd5261-fa52-4c8b-a554-ee1f2653a872"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/"
      ],
      "id": "FAJaSyc89GiB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpp_stratiKfold.csv    test.csv   train.csv.zip\n",
            "sample_submission.csv  train.csv  train_folds_thakur.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03O1cJpp9HAO"
      },
      "source": [
        "### Pretrained RoBERTa Base \n",
        "- Notebook\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "- Model data\n",
        "  - https://www.kaggle.com/maunish/clrp-roberta-base"
      ],
      "id": "03O1cJpp9HAO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFyAMzl9Rs_"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/pretrained-model/\n",
        "!cp -f /content/drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/clrp-model/roberta-pretrained.zip ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "7DFyAMzl9Rs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izsn9ikg99Bn",
        "outputId": "b32c5341-d46c-4716-c736-6a800cf059f6"
      },
      "source": [
        "!unzip -o ../input/commonlitreadabilityprize/pretrained-model/roberta-pretrained.zip -d ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "Izsn9ikg99Bn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../input/commonlitreadabilityprize/pretrained-model/roberta-pretrained.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/merges.txt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/special_tokens_map.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/tokenizer_config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base/vocab.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-600/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base_chk/checkpoint-800/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/text.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGf6ny98zoo"
      },
      "source": [
        ""
      ],
      "id": "AEGf6ny98zoo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020873,
          "end_time": "2021-07-10T16:35:02.554689",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.533816",
          "status": "completed"
        },
        "tags": [],
        "id": "central-liberia"
      },
      "source": [
        "# Overview\n",
        "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
        "\n",
        "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
      ],
      "id": "central-liberia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsgr4s1G-m73",
        "outputId": "48c5f000-5e24-44ae-9d6a-ab3b8d9e8eaa"
      },
      "source": [
        "#ONLY COLAB\n",
        "!pip install transformers accelerate datasets"
      ],
      "id": "Dsgr4s1G-m73",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyaml>=20.4.0\n",
            "  Downloading pyaml-20.4.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.7.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 16.5 MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.2.3)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tqdm, xxhash, pyaml, huggingface-hub, fsspec, datasets, accelerate\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed accelerate-0.3.0 datasets-1.10.2 fsspec-2021.7.0 huggingface-hub-0.0.15 pyaml-20.4.0 tqdm-4.61.2 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:02.605794Z",
          "iopub.status.busy": "2021-07-10T16:35:02.602341Z",
          "iopub.status.idle": "2021-07-10T16:35:11.998468Z",
          "shell.execute_reply": "2021-07-10T16:35:11.999041Z",
          "shell.execute_reply.started": "2021-07-10T16:33:36.630414Z"
        },
        "papermill": {
          "duration": 9.425549,
          "end_time": "2021-07-10T16:35:11.999387",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.573838",
          "status": "completed"
        },
        "tags": [],
        "id": "classical-garage"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "id": "classical-garage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017537,
          "end_time": "2021-07-10T16:35:12.036899",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.019362",
          "status": "completed"
        },
        "tags": [],
        "id": "challenging-bottle"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "challenging-bottle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:12.077789Z",
          "iopub.status.busy": "2021-07-10T16:35:12.076946Z",
          "iopub.status.idle": "2021-07-10T16:35:12.873431Z",
          "shell.execute_reply": "2021-07-10T16:35:12.871700Z"
        },
        "papermill": {
          "duration": 0.818994,
          "end_time": "2021-07-10T16:35:12.873618",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.054624",
          "status": "completed"
        },
        "tags": [],
        "id": "fuzzy-citizen"
      },
      "source": [
        "!"
      ],
      "id": "fuzzy-citizen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:12.966641Z",
          "iopub.status.busy": "2021-07-10T16:35:12.965516Z",
          "iopub.status.idle": "2021-07-10T16:35:12.969362Z",
          "shell.execute_reply": "2021-07-10T16:35:12.968739Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.456435Z"
        },
        "papermill": {
          "duration": 0.076388,
          "end_time": "2021-07-10T16:35:12.969525",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.893137",
          "status": "completed"
        },
        "tags": [],
        "id": "measured-cornwall"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 248\n",
        "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "ROBERTA_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base\"\n",
        "TOKENIZER_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base\"\n",
        "# ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "# TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "measured-cornwall",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.013652Z",
          "iopub.status.busy": "2021-07-10T16:35:13.012651Z",
          "iopub.status.idle": "2021-07-10T16:35:13.016079Z",
          "shell.execute_reply": "2021-07-10T16:35:13.015529Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.470504Z"
        },
        "papermill": {
          "duration": 0.028052,
          "end_time": "2021-07-10T16:35:13.016225",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.988173",
          "status": "completed"
        },
        "tags": [],
        "id": "sitting-brook"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "id": "sitting-brook",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.059695Z",
          "iopub.status.busy": "2021-07-10T16:35:13.059033Z",
          "iopub.status.idle": "2021-07-10T16:35:13.181860Z",
          "shell.execute_reply": "2021-07-10T16:35:13.181008Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.485325Z"
        },
        "papermill": {
          "duration": 0.147365,
          "end_time": "2021-07-10T16:35:13.182094",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.034729",
          "status": "completed"
        },
        "tags": [],
        "id": "banner-plastic"
      },
      "source": [
        "#CHAMGEME\n",
        "# train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
        "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/cpp_stratiKfold.csv\")\n",
        "\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
        "submission_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")"
      ],
      "id": "banner-plastic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.223654Z",
          "iopub.status.busy": "2021-07-10T16:35:13.222955Z",
          "iopub.status.idle": "2021-07-10T16:35:13.465852Z",
          "shell.execute_reply": "2021-07-10T16:35:13.464700Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.537207Z"
        },
        "papermill": {
          "duration": 0.265264,
          "end_time": "2021-07-10T16:35:13.466048",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.200784",
          "status": "completed"
        },
        "tags": [],
        "id": "unavailable-philadelphia"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "id": "unavailable-philadelphia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018281,
          "end_time": "2021-07-10T16:35:13.502997",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.484716",
          "status": "completed"
        },
        "tags": [],
        "id": "intermediate-brand"
      },
      "source": [
        "# Dataset"
      ],
      "id": "intermediate-brand"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.549331Z",
          "iopub.status.busy": "2021-07-10T16:35:13.548372Z",
          "iopub.status.idle": "2021-07-10T16:35:13.552476Z",
          "shell.execute_reply": "2021-07-10T16:35:13.551942Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.797452Z"
        },
        "papermill": {
          "duration": 0.031082,
          "end_time": "2021-07-10T16:35:13.552607",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.521525",
          "status": "completed"
        },
        "tags": [],
        "id": "adopted-prayer"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df        \n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
        "        \n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
        "    \n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = 'max_length',            \n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask=True\n",
        "        )        \n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
        "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
        "        \n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)            \n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "id": "adopted-prayer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018016,
          "end_time": "2021-07-10T16:35:13.588706",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.570690",
          "status": "completed"
        },
        "tags": [],
        "id": "sonic-cooperative"
      },
      "source": [
        "# Model\n",
        "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
      ],
      "id": "sonic-cooperative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.662281Z",
          "iopub.status.busy": "2021-07-10T16:35:13.661368Z",
          "iopub.status.idle": "2021-07-10T16:35:13.679333Z",
          "shell.execute_reply": "2021-07-10T16:35:13.680266Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.811644Z"
        },
        "papermill": {
          "duration": 0.063207,
          "end_time": "2021-07-10T16:35:13.680545",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.617338",
          "status": "completed"
        },
        "tags": [],
        "id": "listed-coordinate"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})                       \n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
        "            \n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 512),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )        \n",
        "\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(768, 1)                        \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_output = self.roberta(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)        \n",
        "\n",
        "        # There are a total of 13 layers of hidden states.\n",
        "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
        "        # We take the hidden states from the last Roberta layer.\n",
        "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        # The number of cells is MAX_LEN.\n",
        "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
        "        # In order to condense hidden states of all cells to a context vector,\n",
        "        # we compute a weighted average of the hidden states of all cells.\n",
        "        # We compute the weight of each cell, using the attention neural network.\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "                \n",
        "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
        "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
        "        # Now we compute context_vector as the weighted average.\n",
        "        # context_vector.shape is BATCH_SIZE x 768\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
        "        \n",
        "        # Now we reduce the context vector to the prediction score.\n",
        "        return self.regressor(context_vector)"
      ],
      "id": "listed-coordinate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.776053Z",
          "iopub.status.busy": "2021-07-10T16:35:13.774791Z",
          "iopub.status.idle": "2021-07-10T16:35:13.782919Z",
          "shell.execute_reply": "2021-07-10T16:35:13.784211Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.831662Z"
        },
        "papermill": {
          "duration": 0.069033,
          "end_time": "2021-07-10T16:35:13.784378",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.715345",
          "status": "completed"
        },
        "tags": [],
        "id": "marked-citation"
      },
      "source": [
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "id": "marked-citation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.860639Z",
          "iopub.status.busy": "2021-07-10T16:35:13.859518Z",
          "iopub.status.idle": "2021-07-10T16:35:13.865246Z",
          "shell.execute_reply": "2021-07-10T16:35:13.867010Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.844758Z"
        },
        "papermill": {
          "duration": 0.049393,
          "end_time": "2021-07-10T16:35:13.867227",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.817834",
          "status": "completed"
        },
        "tags": [],
        "id": "associate-astrology"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "id": "associate-astrology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.957154Z",
          "iopub.status.busy": "2021-07-10T16:35:13.955894Z",
          "iopub.status.idle": "2021-07-10T16:35:13.970258Z",
          "shell.execute_reply": "2021-07-10T16:35:13.971515Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.863562Z"
        },
        "papermill": {
          "duration": 0.064065,
          "end_time": "2021-07-10T16:35:13.971767",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.907702",
          "status": "completed"
        },
        "tags": [],
        "id": "impressed-minnesota"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "                                            \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "id": "impressed-minnesota",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.053687Z",
          "iopub.status.busy": "2021-07-10T16:35:14.052560Z",
          "iopub.status.idle": "2021-07-10T16:35:14.060346Z",
          "shell.execute_reply": "2021-07-10T16:35:14.062050Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.879987Z"
        },
        "papermill": {
          "duration": 0.054887,
          "end_time": "2021-07-10T16:35:14.062272",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.007385",
          "status": "completed"
        },
        "tags": [],
        "id": "handled-trouble"
      },
      "source": [
        "def create_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "    \n",
        "    roberta_parameters = named_parameters[:197]    \n",
        "    attention_parameters = named_parameters[199:203]\n",
        "    regressor_parameters = named_parameters[203:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "\n",
        "        lr = 2e-5\n",
        "\n",
        "        if layer_num >= 69:        \n",
        "            lr = 5e-5\n",
        "\n",
        "        if layer_num >= 133:\n",
        "            lr = 1e-4\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "id": "handled-trouble",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SjNJPllCHBG"
      },
      "source": [
        "# SEED = 1000\n",
        "# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
        "#     print(fold)\n",
        "#     print('------------')\n",
        "#     print(val_indices)"
      ],
      "id": "9SjNJPllCHBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.144962Z",
          "iopub.status.busy": "2021-07-10T16:35:14.141009Z",
          "iopub.status.idle": "2021-07-10T17:19:19.916633Z",
          "shell.execute_reply": "2021-07-10T17:19:19.915678Z"
        },
        "papermill": {
          "duration": 2645.82007,
          "end_time": "2021-07-10T17:19:19.916828",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.096758",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "internal-filename",
        "outputId": "b5e4956e-970d-488c-ab0d-6bbc5700effa"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = f\"model_{fold + 1}.pth\"\n",
        "        \n",
        "    set_random_seed(SEED + fold)\n",
        "    \n",
        "    # train_dataset = LitDataset(train_df.loc[train_indices])    \n",
        "    # val_dataset = LitDataset(train_df.loc[val_indices]) \n",
        "\n",
        "    #CHANGEME\n",
        "    train_dataset = LitDataset(train_df[train_df['kfold']!=fold])    \n",
        "    val_dataset = LitDataset(train_df[train_df['kfold']==fold])   \n",
        "        \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "        \n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)  \n",
        "    # CHANGEME                      \n",
        "    # LB: 0.468\n",
        "    # scheduler = get_cosine_schedule_with_warmup(\n",
        "    #     optimizer,\n",
        "    #     num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "    #     num_warmup_steps=50)\n",
        "    #LB: 0.466\n",
        "    # scheduler = get_linear_schedule_with_warmup(\n",
        "    #     optimizer,\n",
        "    #     num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "    #     num_warmup_steps=25)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=8) \n",
        "\n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
        "    "
      ],
      "id": "internal-filename",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.98 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9198\n",
            "New best_val_rmse: 0.9198\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7289\n",
            "New best_val_rmse: 0.7289\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.5896\n",
            "New best_val_rmse: 0.5896\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6087\n",
            "Still best_val_rmse: 0.5896 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6123\n",
            "Still best_val_rmse: 0.5896 (from epoch 0)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.6487\n",
            "Still best_val_rmse: 0.5896 (from epoch 0)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5399\n",
            "New best_val_rmse: 0.5399\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5618\n",
            "Still best_val_rmse: 0.5399 (from epoch 0)\n",
            "\n",
            "16 steps took 6.51 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5327\n",
            "New best_val_rmse: 0.5327\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.4855\n",
            "New best_val_rmse: 0.4855\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 23 val_rmse: 0.5028\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 39 val_rmse: 0.4996\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 1 batch_num: 47 val_rmse: 0.5186\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 63 val_rmse: 0.4911\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 71 val_rmse: 0.4986\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 79 val_rmse: 0.5076\n",
            "Still best_val_rmse: 0.4855 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 95 val_rmse: 0.4781\n",
            "New best_val_rmse: 0.4781\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 1 batch_num: 97 val_rmse: 0.4854\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 105 val_rmse: 0.5018\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 121 val_rmse: 0.4892\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 125 val_rmse: 0.4971\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.4971\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "8 steps took 3.32 seconds\n",
            "Epoch: 2 batch_num: 0 val_rmse: 0.4925\n",
            "Still best_val_rmse: 0.4781 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 8 val_rmse: 0.4763\n",
            "New best_val_rmse: 0.4763\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.502\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 30 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 32 val_rmse: 0.4819\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 36 val_rmse: 0.4888\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 40 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 44 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4849\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.5125\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4838\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4847\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.4919\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4715\n",
            "New best_val_rmse: 0.4715\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 88 val_rmse: 0.4804\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 92 val_rmse: 0.4723\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 96 val_rmse: 0.4861\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.505\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4716\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4716\n",
            "Still best_val_rmse: 0.4715 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4714\n",
            "New best_val_rmse: 0.4714\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4712\n",
            "New best_val_rmse: 0.4712\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4713\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4717\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4722\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4723\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4712 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4711735128393599]\n",
            "Mean: 0.4711735128393599\n",
            "\n",
            "Fold 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.89 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.6975\n",
            "New best_val_rmse: 0.6975\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.6342\n",
            "New best_val_rmse: 0.6342\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.5889\n",
            "New best_val_rmse: 0.5889\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.5878\n",
            "New best_val_rmse: 0.5878\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.646\n",
            "Still best_val_rmse: 0.5878 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5831\n",
            "New best_val_rmse: 0.5831\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5155\n",
            "New best_val_rmse: 0.5155\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5096\n",
            "New best_val_rmse: 0.5096\n",
            "\n",
            "16 steps took 6.55 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5147\n",
            "Still best_val_rmse: 0.5096 (from epoch 0)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5648\n",
            "Still best_val_rmse: 0.5096 (from epoch 0)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.522\n",
            "Still best_val_rmse: 0.5096 (from epoch 0)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5298\n",
            "Still best_val_rmse: 0.5096 (from epoch 0)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.4891\n",
            "New best_val_rmse: 0.4891\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 1 batch_num: 71 val_rmse: 0.4817\n",
            "New best_val_rmse: 0.4817\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 1 batch_num: 75 val_rmse: 0.5278\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 91 val_rmse: 0.4923\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.4839\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 103 val_rmse: 0.4972\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 111 val_rmse: 0.5023\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 127 val_rmse: 0.5042\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.4961\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4969\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.4926\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.482\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 30 val_rmse: 0.4823\n",
            "Still best_val_rmse: 0.4817 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 34 val_rmse: 0.4817\n",
            "New best_val_rmse: 0.4817\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.4803\n",
            "New best_val_rmse: 0.4803\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.4818\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4865\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4871\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4803\n",
            "New best_val_rmse: 0.4803\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.5052\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4876\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4861\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4836\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4848\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4913\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4892\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4934\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4821\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4829\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4817\n",
            "Still best_val_rmse: 0.4803 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4795\n",
            "New best_val_rmse: 0.4795\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4791\n",
            "New best_val_rmse: 0.4791\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.479\n",
            "New best_val_rmse: 0.479\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.479 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.479 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.479 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.479 (from epoch 2)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.479 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4711735128393599, 0.47903385936980086]\n",
            "Mean: 0.47510368610458037\n",
            "\n",
            "Fold 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.88 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.8402\n",
            "New best_val_rmse: 0.8402\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.6925\n",
            "New best_val_rmse: 0.6925\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.5832\n",
            "New best_val_rmse: 0.5832\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.5647\n",
            "New best_val_rmse: 0.5647\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.5297\n",
            "New best_val_rmse: 0.5297\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5227\n",
            "New best_val_rmse: 0.5227\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5368\n",
            "Still best_val_rmse: 0.5227 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.4998\n",
            "New best_val_rmse: 0.4998\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 0 batch_num: 136 val_rmse: 0.5255\n",
            "Still best_val_rmse: 0.4998 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 1 batch_num: 11 val_rmse: 0.4827\n",
            "New best_val_rmse: 0.4827\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 15 val_rmse: 0.4893\n",
            "Still best_val_rmse: 0.4827 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5036\n",
            "Still best_val_rmse: 0.4827 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.4757\n",
            "New best_val_rmse: 0.4757\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.4739\n",
            "New best_val_rmse: 0.4739\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 1 batch_num: 39 val_rmse: 0.4888\n",
            "Still best_val_rmse: 0.4739 (from epoch 1)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 1 batch_num: 43 val_rmse: 0.4776\n",
            "Still best_val_rmse: 0.4739 (from epoch 1)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 1 batch_num: 45 val_rmse: 0.4838\n",
            "Still best_val_rmse: 0.4739 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 49 val_rmse: 0.4938\n",
            "Still best_val_rmse: 0.4739 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 1 batch_num: 57 val_rmse: 0.5316\n",
            "Still best_val_rmse: 0.4739 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 73 val_rmse: 0.4622\n",
            "New best_val_rmse: 0.4622\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 1 batch_num: 74 val_rmse: 0.4672\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 1 batch_num: 75 val_rmse: 0.4689\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 1 batch_num: 76 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 1 batch_num: 77 val_rmse: 0.4717\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 1 batch_num: 79 val_rmse: 0.4626\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 1 batch_num: 80 val_rmse: 0.4631\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 1 batch_num: 81 val_rmse: 0.4674\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 1 batch_num: 82 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.472\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.4785\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 1 batch_num: 87 val_rmse: 0.472\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 1 batch_num: 89 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4622 (from epoch 1)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 1 batch_num: 90 val_rmse: 0.4614\n",
            "New best_val_rmse: 0.4614\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 1 batch_num: 91 val_rmse: 0.4623\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 1 batch_num: 92 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "1 steps took 0.403 seconds\n",
            "Epoch: 1 batch_num: 93 val_rmse: 0.4654\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 1 batch_num: 94 val_rmse: 0.4691\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 1 batch_num: 95 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 1 batch_num: 97 val_rmse: 0.4884\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.4725\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 1 batch_num: 103 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 1 batch_num: 105 val_rmse: 0.4998\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 1 batch_num: 113 val_rmse: 0.4919\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 121 val_rmse: 0.4791\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "2 steps took 0.797 seconds\n",
            "Epoch: 1 batch_num: 123 val_rmse: 0.527\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 139 val_rmse: 0.4888\n",
            "Still best_val_rmse: 0.4614 (from epoch 1)\n",
            "\n",
            "4 steps took 1.73 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.4604\n",
            "New best_val_rmse: 0.4604\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 3 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 4 val_rmse: 0.4643\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 2 batch_num: 5 val_rmse: 0.4687\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 8 val_rmse: 0.4876\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 12 val_rmse: 0.469\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 13 val_rmse: 0.4653\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.4636\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 15 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 16 val_rmse: 0.4629\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 17 val_rmse: 0.466\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 20 val_rmse: 0.4953\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 28 val_rmse: 0.4625\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.402 seconds\n",
            "Epoch: 2 batch_num: 29 val_rmse: 0.4651\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 30 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 31 val_rmse: 0.4635\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 32 val_rmse: 0.4611\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 33 val_rmse: 0.465\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 34 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.794 seconds\n",
            "Epoch: 2 batch_num: 36 val_rmse: 0.4909\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 44 val_rmse: 0.4682\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 45 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 47 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 48 val_rmse: 0.4617\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 2 batch_num: 49 val_rmse: 0.4617\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.404 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 51 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 52 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4957\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4705\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.464\n",
            "Still best_val_rmse: 0.4604 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 65 val_rmse: 0.4603\n",
            "New best_val_rmse: 0.4603\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.462\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 67 val_rmse: 0.4689\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4916\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4631\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 79 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 80 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 81 val_rmse: 0.4674\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4674\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 83 val_rmse: 0.4671\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 85 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 87 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.808 seconds\n",
            "Epoch: 2 batch_num: 89 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 91 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 95 val_rmse: 0.4741\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 97 val_rmse: 0.4677\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4654\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 99 val_rmse: 0.4641\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 101 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4649\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 103 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.402 seconds\n",
            "Epoch: 2 batch_num: 104 val_rmse: 0.4698\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 105 val_rmse: 0.4722\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 107 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 109 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "2 steps took 0.796 seconds\n",
            "Epoch: 2 batch_num: 111 val_rmse: 0.4813\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 115 val_rmse: 0.4696\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.405 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.402 seconds\n",
            "Epoch: 2 batch_num: 117 val_rmse: 0.4644\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.397 seconds\n",
            "Epoch: 2 batch_num: 119 val_rmse: 0.4621\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4615\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 121 val_rmse: 0.461\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4609\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.403 seconds\n",
            "Epoch: 2 batch_num: 123 val_rmse: 0.4611\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4612\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 125 val_rmse: 0.4614\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4616\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 127 val_rmse: 0.4619\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4621\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 129 val_rmse: 0.4622\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4623\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 131 val_rmse: 0.4625\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.398 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4626\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 133 val_rmse: 0.4628\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.4 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.463\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.406 seconds\n",
            "Epoch: 2 batch_num: 135 val_rmse: 0.4632\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.401 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4634\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 137 val_rmse: 0.4635\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.403 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 139 val_rmse: 0.4637\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "1 steps took 0.399 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4638\n",
            "Still best_val_rmse: 0.4603 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4711735128393599, 0.47903385936980086, 0.4603317110400746]\n",
            "Mean: 0.4701796944164118\n",
            "\n",
            "Fold 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.88 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.7758\n",
            "New best_val_rmse: 0.7758\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8692\n",
            "Still best_val_rmse: 0.7758 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.641\n",
            "New best_val_rmse: 0.641\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.7027\n",
            "Still best_val_rmse: 0.641 (from epoch 0)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6278\n",
            "New best_val_rmse: 0.6278\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.575\n",
            "New best_val_rmse: 0.575\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5585\n",
            "New best_val_rmse: 0.5585\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6066\n",
            "Still best_val_rmse: 0.5585 (from epoch 0)\n",
            "\n",
            "16 steps took 6.55 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5103\n",
            "New best_val_rmse: 0.5103\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.517\n",
            "Still best_val_rmse: 0.5103 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5151\n",
            "Still best_val_rmse: 0.5103 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.4896\n",
            "New best_val_rmse: 0.4896\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 1 batch_num: 55 val_rmse: 0.4951\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 63 val_rmse: 0.5051\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 79 val_rmse: 0.495\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 1 batch_num: 87 val_rmse: 0.5175\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 103 val_rmse: 0.4916\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 111 val_rmse: 0.4954\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 1 batch_num: 119 val_rmse: 0.5054\n",
            "Still best_val_rmse: 0.4896 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 135 val_rmse: 0.4857\n",
            "New best_val_rmse: 0.4857\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 1 batch_num: 139 val_rmse: 0.4873\n",
            "Still best_val_rmse: 0.4857 (from epoch 1)\n",
            "\n",
            "4 steps took 1.73 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.485\n",
            "New best_val_rmse: 0.485\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4838\n",
            "New best_val_rmse: 0.4838\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4911\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.4913\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.5105\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.4871\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4888\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4965\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4854\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4897\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4905\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4868\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4878\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4932\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.5024\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4922\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4928\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "8 steps took 3.18 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4862\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4857\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4882\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4889\n",
            "Still best_val_rmse: 0.4838 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4711735128393599, 0.47903385936980086, 0.4603317110400746, 0.4837805396866877]\n",
            "Mean: 0.47357990573398073\n",
            "\n",
            "Fold 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.9 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.8766\n",
            "New best_val_rmse: 0.8766\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7174\n",
            "New best_val_rmse: 0.7174\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6764\n",
            "New best_val_rmse: 0.6764\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6507\n",
            "New best_val_rmse: 0.6507\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.5992\n",
            "New best_val_rmse: 0.5992\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5716\n",
            "New best_val_rmse: 0.5716\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5831\n",
            "Still best_val_rmse: 0.5716 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5798\n",
            "Still best_val_rmse: 0.5716 (from epoch 0)\n",
            "\n",
            "16 steps took 6.56 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5617\n",
            "New best_val_rmse: 0.5617\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5784\n",
            "Still best_val_rmse: 0.5617 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5268\n",
            "New best_val_rmse: 0.5268\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5245\n",
            "New best_val_rmse: 0.5245\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5413\n",
            "Still best_val_rmse: 0.5245 (from epoch 1)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.5187\n",
            "New best_val_rmse: 0.5187\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.5525\n",
            "Still best_val_rmse: 0.5187 (from epoch 1)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.5284\n",
            "Still best_val_rmse: 0.5187 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.5118\n",
            "New best_val_rmse: 0.5118\n",
            "\n",
            "16 steps took 6.51 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.5164\n",
            "Still best_val_rmse: 0.5118 (from epoch 1)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.5162\n",
            "Still best_val_rmse: 0.5118 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.5108\n",
            "New best_val_rmse: 0.5108\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.5106\n",
            "New best_val_rmse: 0.5106\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.519\n",
            "Still best_val_rmse: 0.5106 (from epoch 2)\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.5078\n",
            "New best_val_rmse: 0.5078\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.5209\n",
            "Still best_val_rmse: 0.5078 (from epoch 2)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.5178\n",
            "Still best_val_rmse: 0.5078 (from epoch 2)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.5132\n",
            "Still best_val_rmse: 0.5078 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4711735128393599, 0.47903385936980086, 0.4603317110400746, 0.4837805396866877, 0.5077914126539422]\n",
            "Mean: 0.480422207117973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.256017,
          "end_time": "2021-07-10T17:19:20.427150",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.171133",
          "status": "completed"
        },
        "tags": [],
        "id": "exposed-cornell"
      },
      "source": [
        "# Inference"
      ],
      "id": "exposed-cornell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.254779,
          "end_time": "2021-07-10T17:19:20.959408",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.704629",
          "status": "completed"
        },
        "tags": [],
        "id": "pacific-explanation"
      },
      "source": [
        ""
      ],
      "id": "pacific-explanation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:21.485458Z",
          "iopub.status.busy": "2021-07-10T17:19:21.484260Z",
          "iopub.status.idle": "2021-07-10T17:19:21.509269Z",
          "shell.execute_reply": "2021-07-10T17:19:21.508604Z"
        },
        "papermill": {
          "duration": 0.29265,
          "end_time": "2021-07-10T17:19:21.509426",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.216776",
          "status": "completed"
        },
        "tags": [],
        "id": "speaking-authority"
      },
      "source": [
        "test_dataset = LitDataset(test_df, inference_only=True)"
      ],
      "id": "speaking-authority",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:22.019508Z",
          "iopub.status.busy": "2021-07-10T17:19:22.018338Z",
          "iopub.status.idle": "2021-07-10T17:19:51.285549Z",
          "shell.execute_reply": "2021-07-10T17:19:51.284997Z"
        },
        "papermill": {
          "duration": 29.52399,
          "end_time": "2021-07-10T17:19:51.285732",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.761742",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "destroyed-interval",
        "outputId": "7a4ea7f2-6f78-4016-e994-f04605c76efa"
      },
      "source": [
        "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
        "\n",
        "test_dataset = LitDataset(test_df, inference_only=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                         drop_last=False, shuffle=False, num_workers=2)\n",
        "\n",
        "for index in range(len(list_val_rmse)):            \n",
        "    model_path = f\"model_{index + 1}.pth\"\n",
        "    print(f\"\\nUsing {model_path}\")\n",
        "                        \n",
        "    model = LitModel()\n",
        "    model.load_state_dict(torch.load(model_path))    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    all_predictions[index] = predict(model, test_loader)\n",
        "    \n",
        "    del model\n",
        "    gc.collect()"
      ],
      "id": "destroyed-interval",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_1.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_2.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_3.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_4.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:51.815506Z",
          "iopub.status.busy": "2021-07-10T17:19:51.814838Z",
          "iopub.status.idle": "2021-07-10T17:19:52.977737Z",
          "shell.execute_reply": "2021-07-10T17:19:52.977091Z"
        },
        "papermill": {
          "duration": 1.434043,
          "end_time": "2021-07-10T17:19:52.977904",
          "exception": false,
          "start_time": "2021-07-10T17:19:51.543861",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meaningful-petersburg",
        "outputId": "c21ffed8-b824-4158-aa7a-129e7487aedb"
      },
      "source": [
        "predictions = all_predictions.mean(axis=0)\n",
        "submission_df.target = predictions\n",
        "print(submission_df)\n",
        "#submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "meaningful-petersburg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id    target\n",
            "0  c0f722661 -0.537849\n",
            "1  f0953f0a5 -0.667442\n",
            "2  0df072751 -0.360057\n",
            "3  04caf4e0c -2.457707\n",
            "4  0e63f8bea -1.718616\n",
            "5  12537fe78 -1.435629\n",
            "6  965e592c0  0.189914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS-MlzVLkY1i"
      },
      "source": [
        "## Upload model"
      ],
      "id": "KS-MlzVLkY1i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_28uXt0kYSU",
        "outputId": "b214d750-bfbd-44f2-b89d-19188b854a1e"
      },
      "source": [
        "!mkdir -p ./output/\n",
        "!cp -f ./model* ./output/\n",
        "#CHANGEME\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/dataset-metadata.json ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/roberta-base/roberta-base-`TZ=JST-9 date +\"%Y%m%d%H%M%S\"`-sche/\" ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/Roberta-base/Roberta-base-`TZ=JST-9 date +\"%m%d%H%M%S\"`-sche/\" ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p ./output/"
      ],
      "id": "x_28uXt0kYSU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting upload for file model_4.pth\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 477M/477M [00:09<00:00, 53.0MB/s]\n",
            "Upload successful: model_4.pth (477MB)\n",
            "Starting upload for file model_3.pth\n",
            "100% 477M/477M [00:09<00:00, 50.1MB/s]\n",
            "Upload successful: model_3.pth (477MB)\n",
            "Starting upload for file model_5.pth\n",
            "100% 477M/477M [00:10<00:00, 47.6MB/s]\n",
            "Upload successful: model_5.pth (477MB)\n",
            "Starting upload for file model_2.pth\n",
            "100% 477M/477M [00:09<00:00, 52.2MB/s]\n",
            "Upload successful: model_2.pth (477MB)\n",
            "Starting upload for file model_1.pth\n",
            "100% 477M/477M [00:10<00:00, 48.0MB/s]\n",
            "Upload successful: model_1.pth (477MB)\n",
            "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//iamnishipy/roberta-base-20210730015756-sche\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD1Cot2sk7A_",
        "outputId": "9205b9ce-c535-4e99-d1b3-e359c29fb3dc"
      },
      "source": [
        "!cat ./output/dataset-metadata.json"
      ],
      "id": "iD1Cot2sk7A_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC0-1.0\"\n",
            "    }\n",
            "  ], \n",
            "  \"id\": \"iamnishipy/roberta-base-20210730015756-sche\", \n",
            "  \"title\": \"Roberta-base-0730015756-sche\"\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
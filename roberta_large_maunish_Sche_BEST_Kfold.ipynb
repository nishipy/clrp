{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2702.979398,
      "end_time": "2021-07-10T17:19:56.452391",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-10T16:34:53.472993",
      "version": "2.3.3"
    },
    "colab": {
      "name": "roberta-large-maunish-Sche-BEST-Kfold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbX1a6A8Doq"
      },
      "source": [
        "# Prerequisite"
      ],
      "id": "UrbX1a6A8Doq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDxPgc_8SBn",
        "outputId": "ce40585d-965d-4595-833b-cde24265e668"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "YdDxPgc_8SBn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-21rfg88O1A",
        "outputId": "3572ceef-8698-4de0-cf5a-341965036b2f"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "Q-21rfg88O1A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug  2 13:29:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMaOL7upRq2w"
      },
      "source": [
        "## Install same version of library as Kaggle Notebook"
      ],
      "id": "KMaOL7upRq2w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBS2Zv7DR2tk",
        "outputId": "4d1c4d9a-7e14-49aa-90f4-4f58460d24fe"
      },
      "source": [
        "!cp -f /content/drive/MyDrive/kaggle/commonlit/roberta-large/requirements.txt ./\n",
        "!cat ./requirements.txt"
      ],
      "id": "LBS2Zv7DR2tk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#pandas==1.2.3\n",
            "sklearn==0.0\n",
            "sklearn-pandas==2.1.0\n",
            "torch==1.7.0\n",
            "torchmetrics==0.2.0\n",
            "#torchtext==0.8.0a0+cd6902d\n",
            "torchvision==0.8.1\n",
            "transformers==4.5.1\n",
            "datasets==1.9.0\n",
            "accelerate==0.3.0"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7y1hmfoS-Q1"
      },
      "source": [
        "# !pip uninstall -r ./requirements.txt -y"
      ],
      "id": "k7y1hmfoS-Q1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhM67RrVQYws"
      },
      "source": [
        "# !pip install -r ./requirements.txt "
      ],
      "id": "DhM67RrVQYws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBMkIBcxUIb2"
      },
      "source": [
        "# !pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"
      ],
      "id": "HBMkIBcxUIb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVVVEFxCNptq",
        "outputId": "65cc65ef-e2ae-40ba-db64-1c53f594c59d"
      },
      "source": [
        "!pip install transformers accelerate datasets"
      ],
      "id": "sVVVEFxCNptq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.7 MB/s \n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Collecting pyaml>=20.4.0\n",
            "  Downloading pyaml-20.4.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tqdm, pyyaml, xxhash, tokenizers, sacremoses, pyaml, huggingface-hub, fsspec, transformers, datasets, accelerate\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed accelerate-0.3.0 datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.12 pyaml-20.4.0 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.62.0 transformers-4.9.1 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2yfzg48VPx"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "De2yfzg48VPx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZk0yJH8mWO"
      },
      "source": [
        "### kaggle.json"
      ],
      "id": "_YZk0yJH8mWO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEuB9fOD8j8l"
      },
      "source": [
        "!mkdir -p /root/.kaggle/\n",
        "!cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "DEuB9fOD8j8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKBPd9H8q0L"
      },
      "source": [
        "### Competition dataset"
      ],
      "id": "4oKBPd9H8q0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv4fLHsg8vsO",
        "outputId": "9bcd6d72-ef11-4ae6-aff4-da382333b62f"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/\n",
        "!kaggle competitions download -c commonlitreadabilityprize -p ../input/commonlitreadabilityprize/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/train_stratiKfold.csv.zip ../input/commonlitreadabilityprize/\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/cpp_stratiKfold.csv ../input/commonlitreadabilityprize/"
      ],
      "id": "lv4fLHsg8vsO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/6.79k [00:00<?, ?B/s]\n",
            "100% 6.79k/6.79k [00:00<00:00, 5.63MB/s]\n",
            "Downloading sample_submission.csv to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/108 [00:00<?, ?B/s]\n",
            "100% 108/108 [00:00<00:00, 91.6kB/s]\n",
            "Downloading train.csv.zip to ../input/commonlitreadabilityprize\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 75.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1QcZXR79EO2",
        "outputId": "dfeeeddd-d404-4ec4-84ed-02f0400cbeae"
      },
      "source": [
        "!unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n",
        "!unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"
      ],
      "id": "R1QcZXR79EO2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../input/commonlitreadabilityprize/train.csv.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/train.csv  \n",
            "Archive:  ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/train_stratiKfold.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonb8JHcUOVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad8cf4f-c5dc-4e8d-c963-7306c4c795fc"
      },
      "source": [
        "!mv ./drive/MyDrive/kaggle/commonlit/train_folds_thakur.csv ../input/commonlitreadabilityprize/"
      ],
      "id": "zonb8JHcUOVk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat './drive/MyDrive/kaggle/commonlit/train_folds_thakur.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJaSyc89GiB",
        "outputId": "593f9453-4aa5-467c-d930-8d52256986ad"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/"
      ],
      "id": "FAJaSyc89GiB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpp_stratiKfold.csv    train.csv\t      train_stratiKfold.csv.zip\n",
            "sample_submission.csv  train.csv.zip\n",
            "test.csv\t       train_stratiKfold.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03O1cJpp9HAO"
      },
      "source": [
        "### Pretrained RoBERTa Large \n",
        "- Pretrain RoBERTa Large in the same way as this notebook\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "- Dataset:\n",
        "  - https://www.kaggle.com/iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "03O1cJpp9HAO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFyAMzl9Rs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324d2cf0-6708-4017-91d0-40552daabc18"
      },
      "source": [
        "!mkdir -p ../input/commonlitreadabilityprize/pretrained-model/\n",
        "!kaggle datasets download iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "7DFyAMzl9Rs_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading roberta-large-20210712191259-mlm.zip to /content\n",
            "100% 8.52G/8.53G [01:38<00:00, 95.8MB/s]\n",
            "100% 8.53G/8.53G [01:38<00:00, 92.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izsn9ikg99Bn",
        "outputId": "b8d10bf6-2e7c-48c9-8a33-67b393130295"
      },
      "source": [
        "!unzip -o ./roberta-large-20210712191259-mlm.zip -d ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "Izsn9ikg99Bn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./roberta-large-20210712191259-mlm.zip\n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/merges.txt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/special_tokens_map.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/tokenizer_config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large/vocab.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1200/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/config.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/optimizer.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/pytorch_model.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/scheduler.pt  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/trainer_state.json  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large_chk/checkpoint-1600/training_args.bin  \n",
            "  inflating: ../input/commonlitreadabilityprize/pretrained-model/text.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWsjc3K2UJrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1301a43c-d664-4184-b5b0-88de95115b85"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "WWsjc3K2UJrO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clrp_roberta_large  clrp_roberta_large_chk  text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGf6ny98zoo"
      },
      "source": [
        ""
      ],
      "id": "AEGf6ny98zoo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020873,
          "end_time": "2021-07-10T16:35:02.554689",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.533816",
          "status": "completed"
        },
        "tags": [],
        "id": "central-liberia"
      },
      "source": [
        "# Overview\n",
        "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
        "\n",
        "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
      ],
      "id": "central-liberia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsgr4s1G-m73"
      },
      "source": [
        "#!pip install transformers accelerate datasets"
      ],
      "id": "Dsgr4s1G-m73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:02.605794Z",
          "iopub.status.busy": "2021-07-10T16:35:02.602341Z",
          "iopub.status.idle": "2021-07-10T16:35:11.998468Z",
          "shell.execute_reply": "2021-07-10T16:35:11.999041Z",
          "shell.execute_reply.started": "2021-07-10T16:33:36.630414Z"
        },
        "papermill": {
          "duration": 9.425549,
          "end_time": "2021-07-10T16:35:11.999387",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.573838",
          "status": "completed"
        },
        "tags": [],
        "id": "classical-garage"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "id": "classical-garage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017537,
          "end_time": "2021-07-10T16:35:12.036899",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.019362",
          "status": "completed"
        },
        "tags": [],
        "id": "challenging-bottle"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "challenging-bottle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:12.966641Z",
          "iopub.status.busy": "2021-07-10T16:35:12.965516Z",
          "iopub.status.idle": "2021-07-10T16:35:12.969362Z",
          "shell.execute_reply": "2021-07-10T16:35:12.968739Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.456435Z"
        },
        "papermill": {
          "duration": 0.076388,
          "end_time": "2021-07-10T16:35:12.969525",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.893137",
          "status": "completed"
        },
        "tags": [],
        "id": "measured-cornwall"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "MAX_LEN = 248\n",
        "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "ROBERTA_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large\"\n",
        "TOKENIZER_PATH = \"../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large\"\n",
        "# ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "# TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "measured-cornwall",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.013652Z",
          "iopub.status.busy": "2021-07-10T16:35:13.012651Z",
          "iopub.status.idle": "2021-07-10T16:35:13.016079Z",
          "shell.execute_reply": "2021-07-10T16:35:13.015529Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.470504Z"
        },
        "papermill": {
          "duration": 0.028052,
          "end_time": "2021-07-10T16:35:13.016225",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.988173",
          "status": "completed"
        },
        "tags": [],
        "id": "sitting-brook"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "id": "sitting-brook",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.059695Z",
          "iopub.status.busy": "2021-07-10T16:35:13.059033Z",
          "iopub.status.idle": "2021-07-10T16:35:13.181860Z",
          "shell.execute_reply": "2021-07-10T16:35:13.181008Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.485325Z"
        },
        "papermill": {
          "duration": 0.147365,
          "end_time": "2021-07-10T16:35:13.182094",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.034729",
          "status": "completed"
        },
        "tags": [],
        "id": "banner-plastic"
      },
      "source": [
        "#CHAMGEME\n",
        "# train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
        "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/cpp_stratiKfold.csv\")\n",
        "\n",
        "\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
        "submission_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")"
      ],
      "id": "banner-plastic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.223654Z",
          "iopub.status.busy": "2021-07-10T16:35:13.222955Z",
          "iopub.status.idle": "2021-07-10T16:35:13.465852Z",
          "shell.execute_reply": "2021-07-10T16:35:13.464700Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.537207Z"
        },
        "papermill": {
          "duration": 0.265264,
          "end_time": "2021-07-10T16:35:13.466048",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.200784",
          "status": "completed"
        },
        "tags": [],
        "id": "unavailable-philadelphia"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "id": "unavailable-philadelphia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018281,
          "end_time": "2021-07-10T16:35:13.502997",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.484716",
          "status": "completed"
        },
        "tags": [],
        "id": "intermediate-brand"
      },
      "source": [
        "# Dataset"
      ],
      "id": "intermediate-brand"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.549331Z",
          "iopub.status.busy": "2021-07-10T16:35:13.548372Z",
          "iopub.status.idle": "2021-07-10T16:35:13.552476Z",
          "shell.execute_reply": "2021-07-10T16:35:13.551942Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.797452Z"
        },
        "papermill": {
          "duration": 0.031082,
          "end_time": "2021-07-10T16:35:13.552607",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.521525",
          "status": "completed"
        },
        "tags": [],
        "id": "adopted-prayer"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df        \n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
        "        \n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
        "    \n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = 'max_length',            \n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask=True\n",
        "        )        \n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
        "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
        "        \n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)            \n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "id": "adopted-prayer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018016,
          "end_time": "2021-07-10T16:35:13.588706",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.570690",
          "status": "completed"
        },
        "tags": [],
        "id": "sonic-cooperative"
      },
      "source": [
        "# Model\n",
        "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
      ],
      "id": "sonic-cooperative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.662281Z",
          "iopub.status.busy": "2021-07-10T16:35:13.661368Z",
          "iopub.status.idle": "2021-07-10T16:35:13.679333Z",
          "shell.execute_reply": "2021-07-10T16:35:13.680266Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.811644Z"
        },
        "papermill": {
          "duration": 0.063207,
          "end_time": "2021-07-10T16:35:13.680545",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.617338",
          "status": "completed"
        },
        "tags": [],
        "id": "listed-coordinate"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})                       \n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n",
        "        #https://towardsdatascience.com/attention-based-deep-multiple-instance-learning-1bb3df857e24\n",
        "        # 768: node fully connected layer\n",
        "        # 512: node attention layer\n",
        "        # self.attention = nn.Sequential(            \n",
        "        #     nn.Linear(768, 512),            \n",
        "        #     nn.Tanh(),                       \n",
        "        #     nn.Linear(512, 1),\n",
        "        #     nn.Softmax(dim=1)\n",
        "        # )        \n",
        "\n",
        "        # self.regressor = nn.Sequential(                        \n",
        "        #     nn.Linear(768, 1)                        \n",
        "        # )\n",
        "\n",
        "\n",
        "        #768 -> 1024\n",
        "        #512 -> 768\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 768),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(768, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )        \n",
        "\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(1024, 1)                        \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_output = self.roberta(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)        \n",
        "\n",
        "        # There are a total of 13 layers of hidden states.\n",
        "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
        "        # We take the hidden states from the last Roberta layer.\n",
        "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        # The number of cells is MAX_LEN.\n",
        "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
        "        # In order to condense hidden states of all cells to a context vector,\n",
        "        # we compute a weighted average of the hidden states of all cells.\n",
        "        # We compute the weight of each cell, using the attention neural network.\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "                \n",
        "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
        "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
        "        # Now we compute context_vector as the weighted average.\n",
        "        # context_vector.shape is BATCH_SIZE x 768\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
        "        \n",
        "        # Now we reduce the context vector to the prediction score.\n",
        "        return self.regressor(context_vector)"
      ],
      "id": "listed-coordinate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.776053Z",
          "iopub.status.busy": "2021-07-10T16:35:13.774791Z",
          "iopub.status.idle": "2021-07-10T16:35:13.782919Z",
          "shell.execute_reply": "2021-07-10T16:35:13.784211Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.831662Z"
        },
        "papermill": {
          "duration": 0.069033,
          "end_time": "2021-07-10T16:35:13.784378",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.715345",
          "status": "completed"
        },
        "tags": [],
        "id": "marked-citation"
      },
      "source": [
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "id": "marked-citation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.860639Z",
          "iopub.status.busy": "2021-07-10T16:35:13.859518Z",
          "iopub.status.idle": "2021-07-10T16:35:13.865246Z",
          "shell.execute_reply": "2021-07-10T16:35:13.867010Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.844758Z"
        },
        "papermill": {
          "duration": 0.049393,
          "end_time": "2021-07-10T16:35:13.867227",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.817834",
          "status": "completed"
        },
        "tags": [],
        "id": "associate-astrology"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "id": "associate-astrology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.957154Z",
          "iopub.status.busy": "2021-07-10T16:35:13.955894Z",
          "iopub.status.idle": "2021-07-10T16:35:13.970258Z",
          "shell.execute_reply": "2021-07-10T16:35:13.971515Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.863562Z"
        },
        "papermill": {
          "duration": 0.064065,
          "end_time": "2021-07-10T16:35:13.971767",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.907702",
          "status": "completed"
        },
        "tags": [],
        "id": "impressed-minnesota"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "                                            \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "id": "impressed-minnesota",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.053687Z",
          "iopub.status.busy": "2021-07-10T16:35:14.052560Z",
          "iopub.status.idle": "2021-07-10T16:35:14.060346Z",
          "shell.execute_reply": "2021-07-10T16:35:14.062050Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.879987Z"
        },
        "papermill": {
          "duration": 0.054887,
          "end_time": "2021-07-10T16:35:14.062272",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.007385",
          "status": "completed"
        },
        "tags": [],
        "id": "handled-trouble"
      },
      "source": [
        "#怪しい\n",
        "def create_optimizer(model):\n",
        "    #model.named_parameters():\n",
        "    #Base -> 205\n",
        "    #Large -> 397\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    #Base\n",
        "    # roberta_parameters = named_parameters[:197]    \n",
        "    # attention_parameters = named_parameters[199:203]\n",
        "    # regressor_parameters = named_parameters[203:]\n",
        "    \n",
        "    #Large\n",
        "    roberta_parameters = named_parameters[:389]    \n",
        "    attention_parameters = named_parameters[391:395]\n",
        "    regressor_parameters = named_parameters[395:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "        lr = 2e-5\n",
        "        #roberta-base: \n",
        "        # if layer_num >= 69: #4/12layers       \n",
        "        #     lr = 5e-5\n",
        "        # if layer_num >= 1f33: #8/12layers\n",
        "        #     lr = 1e-4\n",
        "        #roberta-large\n",
        "        if layer_num >= 133: #8/24layers     \n",
        "            lr = 5e-5\n",
        "        if layer_num >= 261: #16/24layers\n",
        "            lr = 1e-4\n",
        "\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "id": "handled-trouble",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SjNJPllCHBG"
      },
      "source": [
        "# SEED = 1000\n",
        "# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
        "#     print(fold)\n",
        "#     print('------------')\n",
        "#     print(val_indices)"
      ],
      "id": "9SjNJPllCHBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcrbAtOaJaVS"
      },
      "source": [
        "# Debug"
      ],
      "id": "KcrbAtOaJaVS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhG0dfPfJZpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba9dbf8-89c0-4e9f-88df-03993c5caadf"
      },
      "source": [
        "model = LitModel().to(DEVICE)"
      ],
      "id": "vhG0dfPfJZpl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ogBaNfODQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eab8285-16b9-49df-a0e4-a44b64334f7d"
      },
      "source": [
        "named_parameters = list(model.named_parameters())\n",
        "len(named_parameters)"
      ],
      "id": "r9ogBaNfODQ8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGlz9UyNEPt"
      },
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     print(name)"
      ],
      "id": "XtGlz9UyNEPt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxgwubuBJcY1"
      },
      "source": [
        ""
      ],
      "id": "MxgwubuBJcY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMrGZvBFJcOe"
      },
      "source": [
        ""
      ],
      "id": "nMrGZvBFJcOe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtUUBp-MJXoa"
      },
      "source": [
        "# Train\n"
      ],
      "id": "jtUUBp-MJXoa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.144962Z",
          "iopub.status.busy": "2021-07-10T16:35:14.141009Z",
          "iopub.status.idle": "2021-07-10T17:19:19.916633Z",
          "shell.execute_reply": "2021-07-10T17:19:19.915678Z"
        },
        "papermill": {
          "duration": 2645.82007,
          "end_time": "2021-07-10T17:19:19.916828",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.096758",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "internal-filename",
        "outputId": "08d14cea-e0be-40b7-d3ca-24d51576a73e"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = f\"model_{fold + 1}.pth\"\n",
        "        \n",
        "    set_random_seed(SEED + fold)\n",
        "\n",
        "    #CHANGEME\n",
        "    # train_dataset = LitDataset(train_df.loc[train_indices])    \n",
        "    # val_dataset = LitDataset(train_df.loc[val_indices])\n",
        "    train_dataset = LitDataset(train_df[train_df['kfold']!=fold])    \n",
        "    val_dataset = LitDataset(train_df[train_df['kfold']==fold])  \n",
        "        \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "        \n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)                        \n",
        "    # scheduler = get_cosine_schedule_with_warmup(\n",
        "    #     optimizer,\n",
        "    #     num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "    #     num_warmup_steps=50)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=250)\n",
        "    \n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
        "    "
      ],
      "id": "internal-filename",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 12.7 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.023\n",
            "New best_val_rmse: 1.023\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.9212\n",
            "New best_val_rmse: 0.9212\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7705\n",
            "New best_val_rmse: 0.7705\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6869\n",
            "New best_val_rmse: 0.6869\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6287\n",
            "New best_val_rmse: 0.6287\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.698\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6793\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 1.017\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6225\n",
            "New best_val_rmse: 0.6225\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.7199\n",
            "Still best_val_rmse: 0.6225 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.7043\n",
            "Still best_val_rmse: 0.6225 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.6451\n",
            "Still best_val_rmse: 0.6225 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6418\n",
            "Still best_val_rmse: 0.6225 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.6244\n",
            "Still best_val_rmse: 0.6225 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.5829\n",
            "New best_val_rmse: 0.5829\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.6774\n",
            "Still best_val_rmse: 0.5829 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.732\n",
            "Still best_val_rmse: 0.5829 (from epoch 0)\n",
            "\n",
            "16 steps took 11.9 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.7375\n",
            "Still best_val_rmse: 0.5829 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.6182\n",
            "Still best_val_rmse: 0.5829 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.7032\n",
            "Still best_val_rmse: 0.5829 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5613\n",
            "New best_val_rmse: 0.5613\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.558\n",
            "New best_val_rmse: 0.558\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.5661\n",
            "Still best_val_rmse: 0.558 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5996\n",
            "Still best_val_rmse: 0.558 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.5348\n",
            "New best_val_rmse: 0.5348\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.5757\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.6484\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.5765\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5492\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.5491\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 0.5515\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 0.6451\n",
            "Still best_val_rmse: 0.5348 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 0.5248\n",
            "New best_val_rmse: 0.5248\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 0.508\n",
            "New best_val_rmse: 0.508\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.5159\n",
            "Still best_val_rmse: 0.508 (from epoch 1)\n",
            "\n",
            "16 steps took 12.0 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4907\n",
            "New best_val_rmse: 0.4907\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.5219\n",
            "Still best_val_rmse: 0.4907 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 34 val_rmse: 0.4824\n",
            "New best_val_rmse: 0.4824\n",
            "\n",
            "4 steps took 2.95 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.5386\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4875\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4828\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.491\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.5092\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.504\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4896\n",
            "Still best_val_rmse: 0.4824 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4822\n",
            "New best_val_rmse: 0.4822\n",
            "\n",
            "4 steps took 2.95 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4918\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4856\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4892\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4914\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4795\n",
            "New best_val_rmse: 0.4795\n",
            "\n",
            "2 steps took 1.47 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4775\n",
            "New best_val_rmse: 0.4775\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4767\n",
            "New best_val_rmse: 0.4767\n",
            "\n",
            "2 steps took 1.47 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4767 (from epoch 2)\n",
            "\n",
            "2 steps took 1.45 seconds\n",
            "Epoch: 2 batch_num: 142 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.4767 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 144 val_rmse: 0.4766\n",
            "New best_val_rmse: 0.4766\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 146 val_rmse: 0.4765\n",
            "New best_val_rmse: 0.4765\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 148 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.4765 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 150 val_rmse: 0.478\n",
            "Still best_val_rmse: 0.4765 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 152 val_rmse: 0.4756\n",
            "New best_val_rmse: 0.4756\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 0.4743\n",
            "New best_val_rmse: 0.4743\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 156 val_rmse: 0.4739\n",
            "New best_val_rmse: 0.4739\n",
            "\n",
            "2 steps took 1.47 seconds\n",
            "Epoch: 2 batch_num: 158 val_rmse: 0.473\n",
            "New best_val_rmse: 0.473\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 160 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 162 val_rmse: 0.4792\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 164 val_rmse: 0.4914\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 172 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 174 val_rmse: 0.4802\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 178 val_rmse: 0.491\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.4877\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 190 val_rmse: 0.5035\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 206 val_rmse: 0.4745\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 208 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 210 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.473 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 212 val_rmse: 0.4729\n",
            "New best_val_rmse: 0.4729\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 214 val_rmse: 0.4732\n",
            "Still best_val_rmse: 0.4729 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 216 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.4729 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.4729 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 220 val_rmse: 0.4722\n",
            "New best_val_rmse: 0.4722\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 222 val_rmse: 0.4718\n",
            "New best_val_rmse: 0.4718\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 224 val_rmse: 0.4717\n",
            "New best_val_rmse: 0.4717\n",
            "\n",
            "2 steps took 1.47 seconds\n",
            "Epoch: 2 batch_num: 226 val_rmse: 0.4722\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 228 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 230 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 232 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 0.4807\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 238 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 240 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 242 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 244 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 246 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 248 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 252 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 254 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 256 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 258 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 260 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 262 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 264 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 268 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 270 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 272 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 274 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 276 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 278 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 280 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4717 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.47171155464413705]\n",
            "Mean: 0.47171155464413705\n",
            "\n",
            "Fold 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 12.6 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.284\n",
            "New best_val_rmse: 1.284\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8486\n",
            "New best_val_rmse: 0.8486\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7125\n",
            "New best_val_rmse: 0.7125\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6692\n",
            "New best_val_rmse: 0.6692\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.7863\n",
            "Still best_val_rmse: 0.6692 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.6701\n",
            "Still best_val_rmse: 0.6692 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.7749\n",
            "Still best_val_rmse: 0.6692 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6876\n",
            "Still best_val_rmse: 0.6692 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6399\n",
            "New best_val_rmse: 0.6399\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.7974\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 1.121\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 1.051\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 1.058\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 12.0 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 1.085\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 1.073\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 1.054\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 1.055\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 1.049\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 1.041\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 1.046\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 1.04\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 1.064\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 1.041\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 1.057\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.9 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 1.04\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 1.045\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 1.04\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 1.04\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 1.049\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 1.052\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 1.039\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 1.041\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 1.041\n",
            "Still best_val_rmse: 0.6399 (from epoch 0)\n",
            "\n",
            "Performance estimates:\n",
            "[0.47171155464413705, 0.6399280958104813]\n",
            "Mean: 0.5558198252273092\n",
            "\n",
            "Fold 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 12.6 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.078\n",
            "New best_val_rmse: 1.078\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7502\n",
            "New best_val_rmse: 0.7502\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.648\n",
            "New best_val_rmse: 0.648\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.7606\n",
            "Still best_val_rmse: 0.648 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.7757\n",
            "Still best_val_rmse: 0.648 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.8358\n",
            "Still best_val_rmse: 0.648 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6523\n",
            "Still best_val_rmse: 0.648 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5955\n",
            "New best_val_rmse: 0.5955\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6203\n",
            "Still best_val_rmse: 0.5955 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.7471\n",
            "Still best_val_rmse: 0.5955 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.5786\n",
            "New best_val_rmse: 0.5786\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.5774\n",
            "New best_val_rmse: 0.5774\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6416\n",
            "Still best_val_rmse: 0.5774 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.7699\n",
            "Still best_val_rmse: 0.5774 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.6874\n",
            "Still best_val_rmse: 0.5774 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.6789\n",
            "Still best_val_rmse: 0.5774 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.5522\n",
            "New best_val_rmse: 0.5522\n",
            "\n",
            "16 steps took 12.0 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.5761\n",
            "Still best_val_rmse: 0.5522 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.5398\n",
            "New best_val_rmse: 0.5398\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.6155\n",
            "Still best_val_rmse: 0.5398 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5751\n",
            "Still best_val_rmse: 0.5398 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.5326\n",
            "New best_val_rmse: 0.5326\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.569\n",
            "Still best_val_rmse: 0.5326 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5393\n",
            "Still best_val_rmse: 0.5326 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.5331\n",
            "Still best_val_rmse: 0.5326 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.5072\n",
            "New best_val_rmse: 0.5072\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.5329\n",
            "Still best_val_rmse: 0.5072 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.5125\n",
            "Still best_val_rmse: 0.5072 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5648\n",
            "Still best_val_rmse: 0.5072 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.486\n",
            "New best_val_rmse: 0.486\n",
            "\n",
            "4 steps took 2.94 seconds\n",
            "Epoch: 1 batch_num: 201 val_rmse: 0.5854\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 217 val_rmse: 0.5543\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 233 val_rmse: 0.5199\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 249 val_rmse: 0.5226\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 265 val_rmse: 0.4897\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 1 batch_num: 269 val_rmse: 0.4939\n",
            "Still best_val_rmse: 0.486 (from epoch 1)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.481\n",
            "New best_val_rmse: 0.481\n",
            "\n",
            "4 steps took 2.95 seconds\n",
            "Epoch: 1 batch_num: 281 val_rmse: 0.5862\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.9 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.5854\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 30 val_rmse: 0.5045\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.5003\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4967\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.513\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4872\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.5038\n",
            "Still best_val_rmse: 0.481 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4734\n",
            "New best_val_rmse: 0.4734\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 108 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 112 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4798\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4788\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4813\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4893\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.5164\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 146 val_rmse: 0.5095\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 162 val_rmse: 0.481\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 166 val_rmse: 0.4918\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 174 val_rmse: 0.4818\n",
            "Still best_val_rmse: 0.4734 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 178 val_rmse: 0.4722\n",
            "New best_val_rmse: 0.4722\n",
            "\n",
            "2 steps took 1.48 seconds\n",
            "Epoch: 2 batch_num: 180 val_rmse: 0.4721\n",
            "New best_val_rmse: 0.4721\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 182 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 184 val_rmse: 0.4741\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 188 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 190 val_rmse: 0.4819\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "4 steps took 2.92 seconds\n",
            "Epoch: 2 batch_num: 194 val_rmse: 0.4936\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 0.4821\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 206 val_rmse: 0.4721\n",
            "Still best_val_rmse: 0.4721 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 208 val_rmse: 0.4715\n",
            "New best_val_rmse: 0.4715\n",
            "\n",
            "2 steps took 1.5 seconds\n",
            "Epoch: 2 batch_num: 210 val_rmse: 0.4714\n",
            "New best_val_rmse: 0.4714\n",
            "\n",
            "2 steps took 1.45 seconds\n",
            "Epoch: 2 batch_num: 212 val_rmse: 0.4717\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 214 val_rmse: 0.4723\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 216 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 220 val_rmse: 0.4794\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 222 val_rmse: 0.4846\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 226 val_rmse: 0.4837\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 230 val_rmse: 0.4831\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 236 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 238 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 240 val_rmse: 0.4777\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 242 val_rmse: 0.4815\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 246 val_rmse: 0.4846\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 254 val_rmse: 0.48\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 258 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 260 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 262 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 264 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 268 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 270 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 272 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 274 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 276 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 278 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 280 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "2 steps took 1.46 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.4714 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.47171155464413705, 0.6399280958104813, 0.47141258334222164]\n",
            "Mean: 0.52768407793228\n",
            "\n",
            "Fold 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 12.6 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9847\n",
            "New best_val_rmse: 0.9847\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7442\n",
            "New best_val_rmse: 0.7442\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7223\n",
            "New best_val_rmse: 0.7223\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.8304\n",
            "Still best_val_rmse: 0.7223 (from epoch 0)\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.8324\n",
            "Still best_val_rmse: 0.7223 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.7041\n",
            "New best_val_rmse: 0.7041\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6177\n",
            "New best_val_rmse: 0.6177\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.7475\n",
            "Still best_val_rmse: 0.6177 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.7365\n",
            "Still best_val_rmse: 0.6177 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.6133\n",
            "New best_val_rmse: 0.6133\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.7558\n",
            "Still best_val_rmse: 0.6133 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.8235\n",
            "Still best_val_rmse: 0.6133 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6342\n",
            "Still best_val_rmse: 0.6133 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.6303\n",
            "Still best_val_rmse: 0.6133 (from epoch 0)\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.579\n",
            "New best_val_rmse: 0.579\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.6565\n",
            "Still best_val_rmse: 0.579 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.6507\n",
            "Still best_val_rmse: 0.579 (from epoch 0)\n",
            "\n",
            "16 steps took 12.0 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.6343\n",
            "Still best_val_rmse: 0.579 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.591\n",
            "Still best_val_rmse: 0.579 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.6051\n",
            "Still best_val_rmse: 0.579 (from epoch 0)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5604\n",
            "New best_val_rmse: 0.5604\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.5515\n",
            "New best_val_rmse: 0.5515\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.5574\n",
            "Still best_val_rmse: 0.5515 (from epoch 1)\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5492\n",
            "New best_val_rmse: 0.5492\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.5501\n",
            "Still best_val_rmse: 0.5492 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.5581\n",
            "Still best_val_rmse: 0.5492 (from epoch 1)\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.7111\n",
            "Still best_val_rmse: 0.5492 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.5377\n",
            "New best_val_rmse: 0.5377\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5279\n",
            "New best_val_rmse: 0.5279\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.5529\n",
            "Still best_val_rmse: 0.5279 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 0.5283\n",
            "Still best_val_rmse: 0.5279 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 0.5568\n",
            "Still best_val_rmse: 0.5279 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 0.6178\n",
            "Still best_val_rmse: 0.5279 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 0.5558\n",
            "Still best_val_rmse: 0.5279 (from epoch 1)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.5198\n",
            "New best_val_rmse: 0.5198\n",
            "\n",
            "16 steps took 12.0 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.5117\n",
            "New best_val_rmse: 0.5117\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.5195\n",
            "Still best_val_rmse: 0.5117 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.5182\n",
            "Still best_val_rmse: 0.5117 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.5192\n",
            "Still best_val_rmse: 0.5117 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.5068\n",
            "New best_val_rmse: 0.5068\n",
            "\n",
            "16 steps took 11.8 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.5131\n",
            "Still best_val_rmse: 0.5068 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4987\n",
            "New best_val_rmse: 0.4987\n",
            "\n",
            "8 steps took 5.89 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.5035\n",
            "Still best_val_rmse: 0.4987 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.5287\n",
            "Still best_val_rmse: 0.4987 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 146 val_rmse: 0.5037\n",
            "Still best_val_rmse: 0.4987 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 162 val_rmse: 0.4829\n",
            "New best_val_rmse: 0.4829\n",
            "\n",
            "4 steps took 2.94 seconds\n",
            "Epoch: 2 batch_num: 166 val_rmse: 0.5001\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 182 val_rmse: 0.4842\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.4972\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 194 val_rmse: 0.4963\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 0.4867\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 206 val_rmse: 0.4919\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.86 seconds\n",
            "Epoch: 2 batch_num: 214 val_rmse: 0.5107\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "16 steps took 11.7 seconds\n",
            "Epoch: 2 batch_num: 230 val_rmse: 0.4915\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 238 val_rmse: 0.4892\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 242 val_rmse: 0.4889\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "4 steps took 2.93 seconds\n",
            "Epoch: 2 batch_num: 246 val_rmse: 0.4908\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 254 val_rmse: 0.491\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 262 val_rmse: 0.4944\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n",
            "Epoch: 2 batch_num: 270 val_rmse: 0.4977\n",
            "Still best_val_rmse: 0.4829 (from epoch 2)\n",
            "\n",
            "8 steps took 5.87 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.256017,
          "end_time": "2021-07-10T17:19:20.427150",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.171133",
          "status": "completed"
        },
        "tags": [],
        "id": "exposed-cornell"
      },
      "source": [
        "# Inference"
      ],
      "id": "exposed-cornell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.254779,
          "end_time": "2021-07-10T17:19:20.959408",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.704629",
          "status": "completed"
        },
        "tags": [],
        "id": "pacific-explanation"
      },
      "source": [
        ""
      ],
      "id": "pacific-explanation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:21.485458Z",
          "iopub.status.busy": "2021-07-10T17:19:21.484260Z",
          "iopub.status.idle": "2021-07-10T17:19:21.509269Z",
          "shell.execute_reply": "2021-07-10T17:19:21.508604Z"
        },
        "papermill": {
          "duration": 0.29265,
          "end_time": "2021-07-10T17:19:21.509426",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.216776",
          "status": "completed"
        },
        "tags": [],
        "id": "speaking-authority"
      },
      "source": [
        "test_dataset = LitDataset(test_df, inference_only=True)"
      ],
      "id": "speaking-authority",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:22.019508Z",
          "iopub.status.busy": "2021-07-10T17:19:22.018338Z",
          "iopub.status.idle": "2021-07-10T17:19:51.285549Z",
          "shell.execute_reply": "2021-07-10T17:19:51.284997Z"
        },
        "papermill": {
          "duration": 29.52399,
          "end_time": "2021-07-10T17:19:51.285732",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.761742",
          "status": "completed"
        },
        "tags": [],
        "id": "destroyed-interval"
      },
      "source": [
        "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
        "\n",
        "test_dataset = LitDataset(test_df, inference_only=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                         drop_last=False, shuffle=False, num_workers=2)\n",
        "\n",
        "for index in range(len(list_val_rmse)):            \n",
        "    model_path = f\"model_{index + 1}.pth\"\n",
        "    print(f\"\\nUsing {model_path}\")\n",
        "                        \n",
        "    model = LitModel()\n",
        "    model.load_state_dict(torch.load(model_path))    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    all_predictions[index] = predict(model, test_loader)\n",
        "    \n",
        "    del model\n",
        "    gc.collect()"
      ],
      "id": "destroyed-interval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:51.815506Z",
          "iopub.status.busy": "2021-07-10T17:19:51.814838Z",
          "iopub.status.idle": "2021-07-10T17:19:52.977737Z",
          "shell.execute_reply": "2021-07-10T17:19:52.977091Z"
        },
        "papermill": {
          "duration": 1.434043,
          "end_time": "2021-07-10T17:19:52.977904",
          "exception": false,
          "start_time": "2021-07-10T17:19:51.543861",
          "status": "completed"
        },
        "tags": [],
        "id": "meaningful-petersburg"
      },
      "source": [
        "predictions = all_predictions.mean(axis=0)\n",
        "submission_df.target = predictions\n",
        "print(submission_df)\n",
        "#submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "meaningful-petersburg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS-MlzVLkY1i"
      },
      "source": [
        "## Upload model"
      ],
      "id": "KS-MlzVLkY1i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_28uXt0kYSU"
      },
      "source": [
        "!mkdir -p ./output/\n",
        "!cp -f ./model* ./output/\n",
        "#CHANGEME\n",
        "!cp -f ./drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/dataset-metadata.json ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/roberta-base/roberta-large-`TZ=JST-9 date +\"%Y%m%d%H%M%S\"`-sch/\" ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/Roberta-base/Roberta-large-`TZ=JST-9 date +\"%m%d%H%M%S\"`-sch/\" ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p ./output/"
      ],
      "id": "x_28uXt0kYSU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD1Cot2sk7A_"
      },
      "source": [
        "!cat ./output/dataset-metadata.json"
      ],
      "id": "iD1Cot2sk7A_",
      "execution_count": null,
      "outputs": []
    }
  ]
}